{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DACON_컴퓨터비전학습경진대회 연습\n",
    "## 정확도 90%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical CPU, 1 Logical CPU\n"
     ]
    }
   ],
   "source": [
    "# GPU 사용을 위한 코드\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0' # Set to -1 if CPU should be used CPU = -1 , GPU = 0\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "cpus = tf.config.experimental.list_physical_devices('CPU')\n",
    "\n",
    "if gpus:\n",
    "    try:\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)\n",
    "elif cpus:\n",
    "    try:\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        logical_cpus= tf.config.experimental.list_logical_devices('CPU')\n",
    "        print(len(cpus), \"Physical CPU,\", len(logical_cpus), \"Logical CPU\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "train = pd.read_csv('D:/DACON_MNIST/data/train.csv')\n",
    "test = pd.read_csv('D:/DACON_MNIST/data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id  digit letter  0  1  2  3  4  5  6  ...  774  775  776  777  778  779  \\\n",
      "0   1      5      L  1  1  1  4  3  0  0  ...    2    1    0    1    2    4   \n",
      "1   2      0      B  0  4  0  0  4  1  1  ...    0    3    0    1    4    1   \n",
      "2   3      4      L  1  1  2  2  1  1  1  ...    3    3    3    0    2    0   \n",
      "3   4      9      D  1  2  0  2  0  4  0  ...    3    3    2    0    1    4   \n",
      "4   5      6      A  3  0  2  4  0  3  0  ...    4    4    3    2    1    3   \n",
      "\n",
      "   780  781  782  783  \n",
      "0    4    4    3    4  \n",
      "1    4    2    1    2  \n",
      "2    3    0    2    2  \n",
      "3    0    0    1    1  \n",
      "4    4    3    1    2  \n",
      "\n",
      "[5 rows x 787 columns]\n",
      "     id letter  0  1  2  3  4  5  6  7  ...  774  775  776  777  778  779  \\\n",
      "0  2049      L  0  4  0  2  4  2  3  1  ...    2    0    4    2    2    4   \n",
      "1  2050      C  4  1  4  0  1  1  0  2  ...    0    3    2    4    2    4   \n",
      "2  2051      S  0  4  0  1  3  2  3  0  ...    1    3    2    0    3    2   \n",
      "3  2052      K  2  1  3  3  3  4  3  0  ...    3    0    3    2    4    1   \n",
      "4  2053      W  1  0  1  1  2  2  1  4  ...    4    3    1    4    0    2   \n",
      "\n",
      "   780  781  782  783  \n",
      "0    3    4    1    4  \n",
      "1    2    2    1    2  \n",
      "2    3    0    1    4  \n",
      "3    0    4    4    4  \n",
      "4    1    2    3    4  \n",
      "\n",
      "[5 rows x 786 columns]\n",
      "2048\n",
      "20480\n"
     ]
    }
   ],
   "source": [
    "# train data는 id, digit, letter, pixel 로 구성되어 있다.\n",
    "print(train.head(5))\n",
    "# test data는 id, letter, pixel로 구성되어 있다.\n",
    "print(test.head(5))\n",
    "# submission은 id와 digit로 구성되어 있다.\n",
    "print(len(train))\n",
    "print(len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# train 데이터에 있는 이미지 출력해보기(Index에 있는 이미지, 이미지에 해당하는 숫자와 문자)\n",
    "idx = 30\n",
    "img = train.loc[idx, '0':].values.reshape(28, 28).astype(int)\n",
    "digit = train.loc[idx, 'digit']\n",
    "letter = train.loc[idx, 'letter']\n",
    "\n",
    "plt.title('Index: %i, Digit: %s, Letter: %s'%(idx, digit, letter))\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2048, 784)\n",
      "(2048,)\n",
      "(1372, 784)\n",
      "(676, 784)\n",
      "(1372,)\n",
      "(676,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "# 사용하지 않는 column 정보들은 drop한다. (train, test)\n",
    "x_train2 = train.drop(['id', 'digit', 'letter'], axis=1).values\n",
    "y_train2 = train['digit']\n",
    "print(x_train2.shape)\n",
    "print(y_train2.shape)\n",
    "\n",
    "# x_train과 y_train 값을 train, validation set으로 나눈다.\n",
    "x_train2, x_valid2, y_train2, y_valid2 = train_test_split(x_train2, y_train2, test_size=0.33)\n",
    "print(x_train2.shape)\n",
    "print(x_valid2.shape)\n",
    "print(y_train2.shape)\n",
    "print(y_valid2.shape)\n",
    "\n",
    "# train 이미지를 reshape 해준다.\n",
    "x_train2 = x_train2.reshape(-1, 28, 28, 1)\n",
    "x_valid2 = x_valid2.reshape(-1, 28, 28, 1)\n",
    "\n",
    "# train 이미지를 normalization 해준다.\n",
    "x_train2 = x_train2/255\n",
    "x_valid2 = x_valid2/255\n",
    "\n",
    "# y값들은 one_hot encoding 해준다.\n",
    "y_train2 = to_categorical(y_train2, 10)\n",
    "y_valid2 = to_categorical(y_valid2, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "idg_train = ImageDataGenerator(height_shift_range=(-1,1),width_shift_range=(-1,1))\n",
    "idg_valid = ImageDataGenerator()\n",
    "\n",
    "train_generator = idg_train.flow(x_train2, y_train2,batch_size=8)\n",
    "valid_generator = idg_valid.flow(x_valid2, y_valid2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\User\\Anaconda3\\envs\\test\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 28, 28, 16)        160       \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 28, 28, 16)        64        \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 28, 28, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 28, 28, 32)        4640      \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 28, 28, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 28, 28, 32)        25632     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 28, 28, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 28, 28, 32)        25632     \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 28, 28, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 28, 28, 32)        25632     \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 28, 28, 32)        128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 9, 9, 32)          0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 9, 9, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 9, 9, 64)          18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 9, 9, 64)          256       \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 9, 9, 64)          102464    \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 9, 9, 64)          256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 3, 3, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 3, 3, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 576)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 287,274\n",
      "Trainable params: 286,346\n",
      "Non-trainable params: 928\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import *\n",
    "from keras import Sequential\n",
    "\n",
    "model = Sequential()\n",
    "    \n",
    "model.add(Conv2D(16,(3,3),activation='relu',input_shape=(28,28,1),padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.3))\n",
    "    \n",
    "model.add(Conv2D(32,(3,3),activation='relu',padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(32,(5,5),activation='relu',padding='same')) \n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(32,(5,5),activation='relu',padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(32,(5,5),activation='relu',padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D((3,3)))\n",
    "model.add(Dropout(0.3))\n",
    "    \n",
    "model.add(Conv2D(64,(3,3),activation='relu',padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(64,(5,5),activation='relu',padding='same')) \n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D((3,3)))\n",
    "model.add(Dropout(0.3))\n",
    "    \n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(128,activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(64,activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(Dense(10,activation='softmax'))\n",
    "\n",
    "# model.summary()를 통해 모델을 살펴보세요.\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 compile(loss, optimizer 등 설정)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\User\\Anaconda3\\envs\\test\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Epoch 1/200\n",
      "172/172 [==============================] - 14s 79ms/step - loss: 2.9573 - accuracy: 0.1348 - val_loss: 2.5634 - val_accuracy: 0.1050\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 2.56342, saving model to D:/DACON_MNIST/data/model.weights.best.hdf5\n",
      "Epoch 2/200\n",
      "172/172 [==============================] - 13s 74ms/step - loss: 2.4802 - accuracy: 0.1742 - val_loss: 5.0183 - val_accuracy: 0.0947\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 2.56342\n",
      "Epoch 3/200\n",
      "172/172 [==============================] - 13s 75ms/step - loss: 2.2375 - accuracy: 0.2187 - val_loss: 2.3590 - val_accuracy: 0.1257\n",
      "\n",
      "Epoch 00003: val_loss improved from 2.56342 to 2.35895, saving model to D:/DACON_MNIST/data/model.weights.best.hdf5\n",
      "Epoch 4/200\n",
      "172/172 [==============================] - 12s 71ms/step - loss: 2.1525 - accuracy: 0.2413 - val_loss: 1.6524 - val_accuracy: 0.3299\n",
      "\n",
      "Epoch 00004: val_loss improved from 2.35895 to 1.65241, saving model to D:/DACON_MNIST/data/model.weights.best.hdf5\n",
      "Epoch 5/200\n",
      "172/172 [==============================] - 12s 71ms/step - loss: 2.0365 - accuracy: 0.2748 - val_loss: 1.7856 - val_accuracy: 0.4112\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.65241\n",
      "Epoch 6/200\n",
      "172/172 [==============================] - 12s 72ms/step - loss: 2.0569 - accuracy: 0.2748 - val_loss: 1.6560 - val_accuracy: 0.3521\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.65241\n",
      "Epoch 7/200\n",
      "172/172 [==============================] - 13s 75ms/step - loss: 1.9026 - accuracy: 0.3236 - val_loss: 1.7835 - val_accuracy: 0.2885\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.65241\n",
      "Epoch 8/200\n",
      "172/172 [==============================] - 13s 73ms/step - loss: 1.7842 - accuracy: 0.3579 - val_loss: 1.6585 - val_accuracy: 0.4512\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 1.65241\n",
      "Epoch 9/200\n",
      "172/172 [==============================] - 13s 73ms/step - loss: 1.6667 - accuracy: 0.4089 - val_loss: 0.9312 - val_accuracy: 0.5030\n",
      "\n",
      "Epoch 00009: val_loss improved from 1.65241 to 0.93118, saving model to D:/DACON_MNIST/data/model.weights.best.hdf5\n",
      "Epoch 10/200\n",
      "172/172 [==============================] - 13s 76ms/step - loss: 1.5595 - accuracy: 0.4708 - val_loss: 2.5905 - val_accuracy: 0.3920\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.93118\n",
      "Epoch 11/200\n",
      "172/172 [==============================] - 13s 78ms/step - loss: 1.4396 - accuracy: 0.5000 - val_loss: 1.0895 - val_accuracy: 0.4527\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.93118\n",
      "Epoch 12/200\n",
      "172/172 [==============================] - 13s 74ms/step - loss: 1.3406 - accuracy: 0.5372 - val_loss: 0.4448 - val_accuracy: 0.6657\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.93118 to 0.44478, saving model to D:/DACON_MNIST/data/model.weights.best.hdf5\n",
      "Epoch 13/200\n",
      "172/172 [==============================] - 13s 73ms/step - loss: 1.2495 - accuracy: 0.5685 - val_loss: 0.9187 - val_accuracy: 0.7160\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.44478\n",
      "Epoch 14/200\n",
      "172/172 [==============================] - 13s 73ms/step - loss: 1.2076 - accuracy: 0.5700 - val_loss: 1.1120 - val_accuracy: 0.7781\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.44478\n",
      "Epoch 15/200\n",
      "172/172 [==============================] - 13s 73ms/step - loss: 1.1262 - accuracy: 0.6283 - val_loss: 0.3553 - val_accuracy: 0.7175\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.44478 to 0.35535, saving model to D:/DACON_MNIST/data/model.weights.best.hdf5\n",
      "Epoch 16/200\n",
      "172/172 [==============================] - 13s 73ms/step - loss: 1.1477 - accuracy: 0.6268 - val_loss: 0.4386 - val_accuracy: 0.7633\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.35535\n",
      "Epoch 17/200\n",
      "172/172 [==============================] - 12s 71ms/step - loss: 1.0839 - accuracy: 0.6472 - val_loss: 0.9142 - val_accuracy: 0.8033\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.35535\n",
      "Epoch 18/200\n",
      "172/172 [==============================] - 12s 70ms/step - loss: 0.9733 - accuracy: 0.6684 - val_loss: 0.5727 - val_accuracy: 0.8062\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.35535\n",
      "Epoch 19/200\n",
      "172/172 [==============================] - 12s 71ms/step - loss: 0.9556 - accuracy: 0.6808 - val_loss: 0.8409 - val_accuracy: 0.8609\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.35535\n",
      "Epoch 20/200\n",
      "172/172 [==============================] - 12s 71ms/step - loss: 0.9511 - accuracy: 0.6822 - val_loss: 0.0529 - val_accuracy: 0.8432\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.35535 to 0.05291, saving model to D:/DACON_MNIST/data/model.weights.best.hdf5\n",
      "Epoch 21/200\n",
      "172/172 [==============================] - 12s 70ms/step - loss: 0.8697 - accuracy: 0.7223 - val_loss: 1.2241 - val_accuracy: 0.8210\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.05291\n",
      "Epoch 22/200\n",
      "172/172 [==============================] - 12s 70ms/step - loss: 0.8488 - accuracy: 0.7157 - val_loss: 0.4072 - val_accuracy: 0.8802\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.05291\n",
      "Epoch 23/200\n",
      "172/172 [==============================] - 12s 71ms/step - loss: 0.7483 - accuracy: 0.7522 - val_loss: 0.2846 - val_accuracy: 0.8343\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.05291\n",
      "Epoch 24/200\n",
      "172/172 [==============================] - 12s 71ms/step - loss: 0.7800 - accuracy: 0.7485 - val_loss: 0.5706 - val_accuracy: 0.8580\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.05291\n",
      "Epoch 25/200\n",
      "172/172 [==============================] - 12s 71ms/step - loss: 0.8576 - accuracy: 0.7274 - val_loss: 1.3327 - val_accuracy: 0.8343\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.05291\n",
      "Epoch 26/200\n",
      "172/172 [==============================] - 12s 70ms/step - loss: 0.8069 - accuracy: 0.7383 - val_loss: 0.2250 - val_accuracy: 0.8905\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.05291\n",
      "Epoch 27/200\n",
      "172/172 [==============================] - 12s 71ms/step - loss: 0.7404 - accuracy: 0.7617 - val_loss: 0.1100 - val_accuracy: 0.8802\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.05291\n",
      "Epoch 28/200\n",
      "172/172 [==============================] - 12s 70ms/step - loss: 0.7138 - accuracy: 0.7741 - val_loss: 1.0408 - val_accuracy: 0.8905\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.05291\n",
      "Epoch 29/200\n",
      "172/172 [==============================] - 12s 71ms/step - loss: 0.6698 - accuracy: 0.7872 - val_loss: 0.5409 - val_accuracy: 0.8476\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.05291\n",
      "Epoch 30/200\n",
      "172/172 [==============================] - 12s 72ms/step - loss: 0.6748 - accuracy: 0.7813 - val_loss: 0.5926 - val_accuracy: 0.8787\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.05291\n",
      "Epoch 31/200\n",
      "172/172 [==============================] - 12s 72ms/step - loss: 0.6857 - accuracy: 0.7843 - val_loss: 0.2976 - val_accuracy: 0.8905\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.05291\n",
      "Epoch 32/200\n",
      "172/172 [==============================] - 12s 72ms/step - loss: 0.6225 - accuracy: 0.7959 - val_loss: 0.7540 - val_accuracy: 0.8654\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.05291\n",
      "Epoch 33/200\n",
      "172/172 [==============================] - 12s 72ms/step - loss: 0.5962 - accuracy: 0.8061 - val_loss: 0.0611 - val_accuracy: 0.9024\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.05291\n",
      "Epoch 34/200\n",
      "172/172 [==============================] - 12s 71ms/step - loss: 0.6695 - accuracy: 0.7835 - val_loss: 0.1014 - val_accuracy: 0.8817\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.05291\n",
      "Epoch 35/200\n",
      "172/172 [==============================] - 12s 71ms/step - loss: 0.6272 - accuracy: 0.7974 - val_loss: 0.4717 - val_accuracy: 0.9024\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.05291\n",
      "\n",
      "Epoch 00035: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 36/200\n",
      "172/172 [==============================] - 13s 75ms/step - loss: 0.5584 - accuracy: 0.8156 - val_loss: 0.6000 - val_accuracy: 0.8979\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.05291\n",
      "Epoch 37/200\n",
      "172/172 [==============================] - 13s 76ms/step - loss: 0.5251 - accuracy: 0.8280 - val_loss: 0.0370 - val_accuracy: 0.9038\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.05291 to 0.03702, saving model to D:/DACON_MNIST/data/model.weights.best.hdf5\n",
      "Epoch 38/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "172/172 [==============================] - 13s 73ms/step - loss: 0.5162 - accuracy: 0.8367 - val_loss: 0.2315 - val_accuracy: 0.9068\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.03702\n",
      "Epoch 39/200\n",
      "172/172 [==============================] - 12s 72ms/step - loss: 0.5054 - accuracy: 0.8477 - val_loss: 0.5596 - val_accuracy: 0.9201\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.03702\n",
      "Epoch 40/200\n",
      "172/172 [==============================] - 12s 72ms/step - loss: 0.4587 - accuracy: 0.8448 - val_loss: 0.1045 - val_accuracy: 0.9127\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.03702\n",
      "Epoch 41/200\n",
      "172/172 [==============================] - 12s 72ms/step - loss: 0.4808 - accuracy: 0.8484 - val_loss: 0.0382 - val_accuracy: 0.9201\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.03702\n",
      "Epoch 42/200\n",
      "172/172 [==============================] - 12s 72ms/step - loss: 0.4505 - accuracy: 0.8557 - val_loss: 0.2372 - val_accuracy: 0.9083\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.03702\n",
      "Epoch 43/200\n",
      "172/172 [==============================] - 12s 72ms/step - loss: 0.4632 - accuracy: 0.8550 - val_loss: 0.4139 - val_accuracy: 0.9098\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.03702\n",
      "Epoch 44/200\n",
      "172/172 [==============================] - 12s 72ms/step - loss: 0.4488 - accuracy: 0.8637 - val_loss: 0.5619 - val_accuracy: 0.9053\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.03702\n",
      "Epoch 45/200\n",
      "172/172 [==============================] - 12s 72ms/step - loss: 0.4222 - accuracy: 0.8579 - val_loss: 0.0419 - val_accuracy: 0.8994\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.03702\n",
      "Epoch 46/200\n",
      "172/172 [==============================] - 12s 72ms/step - loss: 0.4390 - accuracy: 0.8520 - val_loss: 0.0149 - val_accuracy: 0.9009\n",
      "\n",
      "Epoch 00046: val_loss improved from 0.03702 to 0.01486, saving model to D:/DACON_MNIST/data/model.weights.best.hdf5\n",
      "Epoch 47/200\n",
      "172/172 [==============================] - 12s 72ms/step - loss: 0.3997 - accuracy: 0.8739 - val_loss: 0.0373 - val_accuracy: 0.9024\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.01486\n",
      "Epoch 48/200\n",
      "172/172 [==============================] - 12s 72ms/step - loss: 0.4174 - accuracy: 0.8710 - val_loss: 0.0899 - val_accuracy: 0.9201\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.01486\n",
      "Epoch 49/200\n",
      "172/172 [==============================] - 12s 72ms/step - loss: 0.4145 - accuracy: 0.8601 - val_loss: 1.5057 - val_accuracy: 0.9186\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.01486\n",
      "Epoch 50/200\n",
      "172/172 [==============================] - 12s 72ms/step - loss: 0.4081 - accuracy: 0.8681 - val_loss: 0.3320 - val_accuracy: 0.9053\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.01486\n",
      "Epoch 51/200\n",
      "172/172 [==============================] - 12s 72ms/step - loss: 0.3721 - accuracy: 0.8739 - val_loss: 0.0105 - val_accuracy: 0.9112\n",
      "\n",
      "Epoch 00051: val_loss improved from 0.01486 to 0.01048, saving model to D:/DACON_MNIST/data/model.weights.best.hdf5\n",
      "Epoch 52/200\n",
      "172/172 [==============================] - 12s 72ms/step - loss: 0.3662 - accuracy: 0.8885 - val_loss: 0.2846 - val_accuracy: 0.9068\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.01048\n",
      "Epoch 53/200\n",
      "172/172 [==============================] - 12s 72ms/step - loss: 0.3858 - accuracy: 0.8768 - val_loss: 0.0232 - val_accuracy: 0.9098\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.01048\n",
      "Epoch 54/200\n",
      "172/172 [==============================] - 13s 76ms/step - loss: 0.3970 - accuracy: 0.8746 - val_loss: 0.0430 - val_accuracy: 0.9098\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.01048\n",
      "Epoch 55/200\n",
      "172/172 [==============================] - 14s 79ms/step - loss: 0.3885 - accuracy: 0.8739 - val_loss: 0.0175 - val_accuracy: 0.9068\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.01048\n",
      "Epoch 56/200\n",
      "172/172 [==============================] - 13s 74ms/step - loss: 0.3838 - accuracy: 0.8732 - val_loss: 0.1676 - val_accuracy: 0.9038\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.01048\n",
      "Epoch 57/200\n",
      "172/172 [==============================] - 12s 71ms/step - loss: 0.3434 - accuracy: 0.8885 - val_loss: 0.1026 - val_accuracy: 0.9083\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.01048\n",
      "Epoch 58/200\n",
      "172/172 [==============================] - 12s 71ms/step - loss: 0.3242 - accuracy: 0.9001 - val_loss: 0.1047 - val_accuracy: 0.9201\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.01048\n",
      "Epoch 59/200\n",
      "172/172 [==============================] - 12s 73ms/step - loss: 0.3481 - accuracy: 0.8848 - val_loss: 0.0258 - val_accuracy: 0.9009\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.01048\n",
      "Epoch 60/200\n",
      "172/172 [==============================] - 12s 72ms/step - loss: 0.3155 - accuracy: 0.8994 - val_loss: 0.2219 - val_accuracy: 0.8964\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.01048\n",
      "Epoch 61/200\n",
      "172/172 [==============================] - 12s 71ms/step - loss: 0.3242 - accuracy: 0.8885 - val_loss: 0.3191 - val_accuracy: 0.9068\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.01048\n",
      "Epoch 62/200\n",
      "172/172 [==============================] - 12s 72ms/step - loss: 0.3479 - accuracy: 0.8827 - val_loss: 0.2128 - val_accuracy: 0.9201\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.01048\n",
      "Epoch 63/200\n",
      "172/172 [==============================] - 13s 73ms/step - loss: 0.3153 - accuracy: 0.8914 - val_loss: 0.0772 - val_accuracy: 0.9127\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.01048\n",
      "Epoch 64/200\n",
      "172/172 [==============================] - 12s 72ms/step - loss: 0.3255 - accuracy: 0.8936 - val_loss: 0.0638 - val_accuracy: 0.9038\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.01048\n",
      "Epoch 65/200\n",
      "172/172 [==============================] - 13s 74ms/step - loss: 0.3346 - accuracy: 0.9001 - val_loss: 0.1052 - val_accuracy: 0.9142\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.01048\n",
      "Epoch 66/200\n",
      "172/172 [==============================] - 12s 72ms/step - loss: 0.3237 - accuracy: 0.8965 - val_loss: 0.6749 - val_accuracy: 0.9142\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.01048\n",
      "\n",
      "Epoch 00066: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Epoch 67/200\n",
      "172/172 [==============================] - 13s 74ms/step - loss: 0.2521 - accuracy: 0.9249 - val_loss: 0.0712 - val_accuracy: 0.9201\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.01048\n",
      "Epoch 68/200\n",
      "172/172 [==============================] - 13s 75ms/step - loss: 0.2764 - accuracy: 0.9009 - val_loss: 0.1390 - val_accuracy: 0.9231\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.01048\n",
      "Epoch 69/200\n",
      "172/172 [==============================] - 13s 74ms/step - loss: 0.2360 - accuracy: 0.9169 - val_loss: 0.0087 - val_accuracy: 0.9201\n",
      "\n",
      "Epoch 00069: val_loss improved from 0.01048 to 0.00872, saving model to D:/DACON_MNIST/data/model.weights.best.hdf5\n",
      "Epoch 70/200\n",
      "172/172 [==============================] - 13s 76ms/step - loss: 0.2099 - accuracy: 0.9344 - val_loss: 0.0542 - val_accuracy: 0.9186\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.00872\n",
      "Epoch 71/200\n",
      "172/172 [==============================] - 13s 73ms/step - loss: 0.2469 - accuracy: 0.9206 - val_loss: 0.2682 - val_accuracy: 0.9260\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.00872\n",
      "Epoch 72/200\n",
      "172/172 [==============================] - 12s 72ms/step - loss: 0.2365 - accuracy: 0.9344 - val_loss: 1.1031 - val_accuracy: 0.9275\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.00872\n",
      "Epoch 73/200\n",
      "172/172 [==============================] - 13s 73ms/step - loss: 0.2267 - accuracy: 0.9286 - val_loss: 0.0460 - val_accuracy: 0.9201\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.00872\n",
      "Epoch 74/200\n",
      "172/172 [==============================] - 12s 71ms/step - loss: 0.2229 - accuracy: 0.9191 - val_loss: 7.7870e-04 - val_accuracy: 0.9172\n",
      "\n",
      "Epoch 00074: val_loss improved from 0.00872 to 0.00078, saving model to D:/DACON_MNIST/data/model.weights.best.hdf5\n",
      "Epoch 75/200\n",
      "172/172 [==============================] - 12s 71ms/step - loss: 0.2080 - accuracy: 0.9395 - val_loss: 0.0319 - val_accuracy: 0.9186\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.00078\n",
      "Epoch 76/200\n",
      "172/172 [==============================] - 12s 71ms/step - loss: 0.2368 - accuracy: 0.9322 - val_loss: 0.0394 - val_accuracy: 0.9172\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.00078\n",
      "Epoch 77/200\n",
      "172/172 [==============================] - 12s 71ms/step - loss: 0.2028 - accuracy: 0.9395 - val_loss: 0.0425 - val_accuracy: 0.9260\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00077: val_loss did not improve from 0.00078\n",
      "Epoch 78/200\n",
      "172/172 [==============================] - 12s 72ms/step - loss: 0.2570 - accuracy: 0.9242 - val_loss: 0.0103 - val_accuracy: 0.9201\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.00078\n",
      "Epoch 79/200\n",
      "172/172 [==============================] - 12s 71ms/step - loss: 0.2015 - accuracy: 0.9395 - val_loss: 0.0973 - val_accuracy: 0.9201\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.00078\n",
      "Epoch 80/200\n",
      "172/172 [==============================] - 12s 71ms/step - loss: 0.2287 - accuracy: 0.9308 - val_loss: 0.6289 - val_accuracy: 0.9142\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.00078\n",
      "Epoch 81/200\n",
      "172/172 [==============================] - 13s 73ms/step - loss: 0.2415 - accuracy: 0.9198 - val_loss: 0.0886 - val_accuracy: 0.9186\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.00078\n",
      "Epoch 82/200\n",
      "172/172 [==============================] - 12s 72ms/step - loss: 0.2522 - accuracy: 0.9278 - val_loss: 0.0193 - val_accuracy: 0.9216\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.00078\n",
      "Epoch 83/200\n",
      "172/172 [==============================] - 12s 72ms/step - loss: 0.2145 - accuracy: 0.9322 - val_loss: 0.4592 - val_accuracy: 0.9231\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.00078\n",
      "Epoch 84/200\n",
      "172/172 [==============================] - 13s 73ms/step - loss: 0.2709 - accuracy: 0.9155 - val_loss: 0.3873 - val_accuracy: 0.9216\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.00078\n",
      "Epoch 85/200\n",
      "172/172 [==============================] - 12s 72ms/step - loss: 0.2086 - accuracy: 0.9293 - val_loss: 0.0517 - val_accuracy: 0.9201\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.00078\n",
      "Epoch 86/200\n",
      "172/172 [==============================] - 12s 72ms/step - loss: 0.1941 - accuracy: 0.9410 - val_loss: 0.0124 - val_accuracy: 0.9246\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.00078\n",
      "Epoch 87/200\n",
      "172/172 [==============================] - 12s 72ms/step - loss: 0.2158 - accuracy: 0.9388 - val_loss: 0.8716 - val_accuracy: 0.9186\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.00078\n",
      "Epoch 88/200\n",
      "172/172 [==============================] - 12s 72ms/step - loss: 0.2321 - accuracy: 0.9300 - val_loss: 0.0018 - val_accuracy: 0.9216\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.00078\n",
      "Epoch 89/200\n",
      "172/172 [==============================] - 12s 72ms/step - loss: 0.1917 - accuracy: 0.9431 - val_loss: 0.4093 - val_accuracy: 0.9186\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.00078\n",
      "\n",
      "Epoch 00089: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 90/200\n",
      "172/172 [==============================] - 12s 72ms/step - loss: 0.1703 - accuracy: 0.9497 - val_loss: 0.0085 - val_accuracy: 0.9231\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.00078\n",
      "Epoch 91/200\n",
      "172/172 [==============================] - 12s 72ms/step - loss: 0.1892 - accuracy: 0.9461 - val_loss: 0.0083 - val_accuracy: 0.9201\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.00078\n",
      "Epoch 92/200\n",
      "172/172 [==============================] - 12s 72ms/step - loss: 0.1809 - accuracy: 0.9453 - val_loss: 0.0018 - val_accuracy: 0.9186\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.00078\n",
      "Epoch 93/200\n",
      "172/172 [==============================] - 12s 72ms/step - loss: 0.1642 - accuracy: 0.9461 - val_loss: 0.0023 - val_accuracy: 0.9186\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.00078\n",
      "Epoch 94/200\n",
      "172/172 [==============================] - 12s 73ms/step - loss: 0.1631 - accuracy: 0.9541 - val_loss: 0.1971 - val_accuracy: 0.9231\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.00078\n",
      "Epoch 00094: early stopping\n"
     ]
    }
   ],
   "source": [
    "# 훈련시키면서 가장 정확도가 높은 모델 저장하는 방법 : ModelCheckpoint\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "reLR = ReduceLROnPlateau(patience=15,verbose=1,factor=0.5) #learning rate scheduler\n",
    "es = EarlyStopping(patience=20, verbose=1)\n",
    "checkpointer = ModelCheckpoint(filepath='D:/DACON_MNIST/data/model.weights.best.hdf5', verbose = 1, save_best_only=True)\n",
    "# history = model.fit(x_train2,\n",
    "#           y_train2,\n",
    "#           batch_size=32,\n",
    "#           epochs=200,\n",
    "#           validation_data=(x_valid2, y_valid2),\n",
    "#           callbacks=[checkpointer,es,reLR])\n",
    "history = model.fit_generator(train_generator,\n",
    "          epochs=200,\n",
    "          validation_data=valid_generator,\n",
    "          callbacks=[checkpointer,es,reLR])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 진행한 결과 모델 저장하기\n",
    "from keras.models import load_model\n",
    "\n",
    "model.save('D:/DACON_MNIST/data/DACON_mnist_model1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.94970846, 0.9460641, 0.94533527, 0.9460641, 0.95408165]\n"
     ]
    }
   ],
   "source": [
    "# 모델 학습 결과 그래프로 확인하기\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.title('Model loss & accuracy')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['loss', 'accuracy'], loc='upper left')\n",
    "plt.show()\n",
    "print(history.history['accuracy'][-5:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>digit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2049</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2050</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2051</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2052</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2053</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id  digit\n",
       "0  2049      6\n",
       "1  2050      9\n",
       "2  2051      8\n",
       "3  2052      0\n",
       "4  2053      3"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test 데이터셋에서 x_train과 동일한 조건으로 예측 실행\n",
    "x_test = test.drop(['id', 'letter'], axis=1).values\n",
    "x_test = x_test.reshape(-1, 28, 28, 1)\n",
    "x_test = x_test/255\n",
    "\n",
    "# submission의 'digit'에 예측한 결과값을 적어줌\n",
    "submission = pd.read_csv('D:/DACON_MNIST/data/submission.csv')\n",
    "submission['digit'] = np.argmax(model.predict(x_test), axis=1)\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결과를 csv파일로 저장함\n",
    "submission.to_csv('D:/DACON_MNIST/data/DACON_MNIST_03.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "test"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
