{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DACON_컴퓨터비전학습경진대회 연습\n",
    "## 정확도 90%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical CPU, 1 Logical CPU\n"
     ]
    }
   ],
   "source": [
    "# GPU 사용을 위한 코드\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0' # Set to -1 if CPU should be used CPU = -1 , GPU = 0\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "cpus = tf.config.experimental.list_physical_devices('CPU')\n",
    "\n",
    "if gpus:\n",
    "    try:\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)\n",
    "elif cpus:\n",
    "    try:\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        logical_cpus= tf.config.experimental.list_logical_devices('CPU')\n",
    "        print(len(cpus), \"Physical CPU,\", len(logical_cpus), \"Logical CPU\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "train = pd.read_csv('D:/DACON_MNIST/data/train.csv')\n",
    "test = pd.read_csv('D:/DACON_MNIST/data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id  digit letter  0  1  2  3  4  5  6  ...  774  775  776  777  778  779  \\\n",
      "0   1      5      L  1  1  1  4  3  0  0  ...    2    1    0    1    2    4   \n",
      "1   2      0      B  0  4  0  0  4  1  1  ...    0    3    0    1    4    1   \n",
      "2   3      4      L  1  1  2  2  1  1  1  ...    3    3    3    0    2    0   \n",
      "3   4      9      D  1  2  0  2  0  4  0  ...    3    3    2    0    1    4   \n",
      "4   5      6      A  3  0  2  4  0  3  0  ...    4    4    3    2    1    3   \n",
      "\n",
      "   780  781  782  783  \n",
      "0    4    4    3    4  \n",
      "1    4    2    1    2  \n",
      "2    3    0    2    2  \n",
      "3    0    0    1    1  \n",
      "4    4    3    1    2  \n",
      "\n",
      "[5 rows x 787 columns]\n",
      "     id letter  0  1  2  3  4  5  6  7  ...  774  775  776  777  778  779  \\\n",
      "0  2049      L  0  4  0  2  4  2  3  1  ...    2    0    4    2    2    4   \n",
      "1  2050      C  4  1  4  0  1  1  0  2  ...    0    3    2    4    2    4   \n",
      "2  2051      S  0  4  0  1  3  2  3  0  ...    1    3    2    0    3    2   \n",
      "3  2052      K  2  1  3  3  3  4  3  0  ...    3    0    3    2    4    1   \n",
      "4  2053      W  1  0  1  1  2  2  1  4  ...    4    3    1    4    0    2   \n",
      "\n",
      "   780  781  782  783  \n",
      "0    3    4    1    4  \n",
      "1    2    2    1    2  \n",
      "2    3    0    1    4  \n",
      "3    0    4    4    4  \n",
      "4    1    2    3    4  \n",
      "\n",
      "[5 rows x 786 columns]\n",
      "2048\n",
      "20480\n"
     ]
    }
   ],
   "source": [
    "# train data는 id, digit, letter, pixel 로 구성되어 있다.\n",
    "print(train.head(5))\n",
    "# test data는 id, letter, pixel로 구성되어 있다.\n",
    "print(test.head(5))\n",
    "# submission은 id와 digit로 구성되어 있다.\n",
    "print(len(train))\n",
    "print(len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAEICAYAAACZA4KlAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAaPUlEQVR4nO2df7RdZXnnP8/NT0giJIBpDIEAZlXFjlgjUkWlS20Ra9FO/UGrhRkXsS3Msms5qyozXWY61TKzqi394Y9QKPFnq0sZ0aG1DLXSLJUaXJEfjRIqBEJiAoRAEiTcH8/8sfdtD5e7n+fcs88958L7/ax11jlnv/t997Pfvb9n77Of93kfc3eEEM98RoZtgBBiMEjsQhSCxC5EIUjsQhSCxC5EIUjsQhSCxD4NZrbRzD4zbDtmGzO7w8zO6fe6Ym7yjBW7md1jZq8dth0RZvYNM3vAzB41s++b2flTyn/NzHaa2WEz+z9mtqLLdteamZvZofq118y+Zmav61zP3U9393/sps3OdXv5MeywZfI1bmZ/1mXda8zsD2ayvbrek86Bjn6ZP9O2utzeZR3793i9j5Pf75iNbc6EZ6zYnya8B1jl7s8CNgCfMbNVAGZ2OvBJ4J3ASuAx4GMzbP9Yd18KvAi4AbjWzC7qk+0zwt2XTr6o9ucnwBeHYUuvZD8S7v7hjn38TeDbHft9+mCsbKYIsZvZRWa2xcz+yMweNrO7zez1HeWnmNk3zeygmd0AHD+l/llm9i0zO1Bfgc+pl7/czB40szX19xfV6zyvG7vc/VZ3H5v8CiwA1tTffx34qrvf5O6HgN8DfsXMls10/939x+5+BbAR+F9mNlLb+29XPjM7ysw21/2z3cx+18x2dfTBPWb2WjM7F7gMeFt9xfr+TO0BfhXYB/xTD3WfhJn9kpltq/v9W2b2H+rlnwZOAr5a2/m7wE11tQP1sp+r1/3P9T4/bGZfN7OTO9p3M7vEzHYAO9raO1Tc/Rn5Au4BXlt/vggYBS4G5gG/BewGrC7/NvBRYBHwKuAg8Jm6bDXwEHAe1Y/j6+rvJ9TlHwL+ATgKuBW4tMOGjwEfS+z8GvA4ldj/Dhipl38FeN+UdQ8BL+li39fW7c2fsvzUevnzp+mjy4FvAsuBE+t92dXQnxsn+6ej/P3A17o8Nv8AbJzBsbwG+INplv8s1Y/Gy+rjemFt56KpNjf1C/Am4C7g+cB84L8D3+ood6q7ohXAUfWyA8DZic0XAVuGrYPOVxFX9pqd7n6lu48Dm4FVwEozOwl4KfB77n7E3W8CvtpR7x3A9e5+vbtPuPsNwFYq8UN14h8D/DPVD8hfTFZ0999299+OjHL3XwKW1e193d0n6qKlwCNTVn+kXrdXdtfv0/33fyvwYXd/2N13AX86k4bd/fJ6X0Lq/n411TFoy8XAJ939Zncfd/fNwBHgrBm08W7gD919u1d3WR8Gzui8utfl+939JwDufqy7b+mD/QOlJLH/ePKDuz9Wf1wKPAd42N0Pd6y7s+PzycBb6tvEA2Z2ADib6scCdx+luvK8EPiI1z/rM8HdR939b4FfNLNfrhcfAp41ZdVnUd119Mrq+n3/NGXPAe7r+H7fNOv0g9+guuLd3Ye2TgbeO+XYrKHal5m0cUVH/f2A8e99BbPXFwNlVp5KPs3YAyw3syUdgj+J6vYNqgP9aXe/eLrKZrYa+CDwV8BHzOyl7n6kR1vmA6fVn++gerA2uZ1Tqf5m3Nlj2wBvprrt/eE0ZXuobt//pf6+Zpp1JmkTKvkbVH8Z+sF9wIfc/UMN5VPtnM7uyTY+G2znGREaWtKVfVrcfSfVbfn/MLOFZnY28MaOVT4DvNHMftHM5pnZYjM7x8xONDOjuqpfBbyLSjD/s5vtmtnzzOz19YOxBWb2DqrnBd+sV/lsvd1XmtkS4PeBL7v7wbr+RjP7xy63tdLMLqX6UfpAx1+FTr4AfMDMltc/YJcGTe4F1k4+6OsWM3s51RXzKU/h6wdh5wTVJ/t+8rUQuBL4TTN7mVUsMbM3dDzE3Ev1nGKSB4CJKcs+QbXfp9d2HGNmb5nJfj1tGPZDg9l68dQHdFumlDvw3PrzqVRPhg9RPYz5czoeQFE9APom1S3eA8D/pbr6v4fqQdbCer3n1OWvrL9/AvhEg33PB26mui0/AHwXePOUdX4NuBc4TPXAbkVH2VVUV6Tp2l5b79+huu4+4Hrg3KCPlgCfrm3ZTvWg6l8b1j0O2AI8DHyvXnYZ8LfJMfkk1V3S1OUn1v1wXEO9a+r96XxtqcvOrfvuANWP7ReBZXXZ+XX/HQD+a73s9+tjdAA4q172TuA24FGqK/3V050nHcsOTR7jYF+fcs4N+zX5NFo8zTCzbcBr3P2hWWr/t4C3u/urZ6P9Kdt6B3C6u39gtrdVMhK7AKAezHMqlRtyHdXdy5+7+58M1TDRN/SATkyykOo2+xSqW9y/ZuYj9sQcRld2IQqh+KfxQpTCQG/jF9oiX8yS5hUsayFdoZnsDsaStufyHVBme8Rs71dk2zD7tO25lp5PSfNtdj1o+3E/zBN+ZNo1Wom9Doq4gmpc8l+6ezhYYjFLeJm9prm9+Yk5M3PrPgkfH4+bnjev9/rTuq1nQHbijMS2ZbaHmx59otW2MyLb0m1nP2LZ+TDRfMzanms+NhpXz86nsbHmwux4jzT3y3fGvt7cbNhqtEGzeVTjwF8PvAC4wMxe0Gt7QojZpc1/9jOBu9z9R+7+BNXT2/OTOkKIIdFG7Kt5coDALp4cPACAmW0ws61mtnWUXoeMCyHa0kbs0/1xeMqfT3ff5O7r3X39Aha12JwQog1txL6LJ0dGnci/x0sLIeYYbcT+XWBdPaXTQuDtwHX9MUsI0W96dr25+1gdNvl1Ktfb1e4ez6Bphi1YGDQau7Ai91fm6ojcFd0Q1ffYq9fatZb2y2jQL1F/k7ugfCKxPXM7enO/Zbalrrms44N+zfbLRuL9svkLwvI2Ls30XA3dgs11W/nZ3f16qtBJIcQcR8NlhSgEiV2IQpDYhSgEiV2IQpDYhSgEiV2IQhjstFTu7UJFA/9iFsIahTt2QxoS2abtxK/qY7FPeGTx4ua2l8UJZCYefTQsZzz2F6f+5ui4TCS+6IwWoaDp+IGELMQ1C88NQ3/TcOx4003oyi5EIUjsQhSCxC5EIUjsQhSCxC5EIUjsQhTC4DPCBO61VmGDbaZTpotwy9BlmIRLtpzJdN8lLw/LjxzbXDbykkfCuvP+aV1Yvvqre8LyiZ33h+Vh6HGbGX0hdadGntzWob0ZLabJTl2xPZ6LurILUQgSuxCFILELUQgSuxCFILELUQgSuxCFILELUQiD97O3CVONpt9tkckUWmblHGmZfTYJ7X3kp+N+efn6HzaWXXXyDWHdi1c1Z9UF+M4Jp4flp1x2d1gehqG2CGmuyluEkbadIjshG7cRDwLIzpckvLYBXdmFKASJXYhCkNiFKASJXYhCkNiFKASJXYhCkNiFKITB+9kj0vS/zb7RNHq4pd+01fgAkvLEtoUPx2MI9v6kebrox30srLty0cGwfGxZy35rNYV3u22HvvQW51qXG4/Lo/MpG/MRzfswOkspm83sHuAg1dk85u7r27QnhJg9+nFl/3l3f7AP7QghZhH9ZxeiENqK3YG/N7NbzGzDdCuY2QYz22pmW0c50nJzQoheaXsb/wp3321mzwZuMLMfuPtNnSu4+yZgE8CzbEXLpx5CiF5pdWV39931+z7gWuDMfhglhOg/PYvdzJaY2bLJz8AvALf3yzAhRH9pcxu/ErjWqpji+cDn3P3v0lqR/zGbfz2IEc5T6LZ8PBHZnfiSsznKR45ZHpaf8qX9Yfmeh9Y0lv2nX/3lsO6+x+KUzsf8IPbxH3zbWWG5Bb7u0aPjY7L8U98Jy9vME5DOf9DyfAlzHNAyBXjow2/u75636O4/Al7Ua30hxGCR602IQpDYhSgEiV2IQpDYhSgEiV2IQphbIa5Z2uUW6Z7bTlschbGmrrW1za4xgLGVx4TlD/7MUWH56JLmsu/fclpYl6TLl2WXgyxTdnBMFz8Suyx3XBGP0bKJeOPPu6I53fT47h+HdX00Dg3Oz6cWaZczwnO1ebu6sgtRCBK7EIUgsQtRCBK7EIUgsQtRCBK7EIUgsQtRCIP1s5uF/vAsTNXHAt9ni/S9kIckzlveHIY6+sK1Yd17Xx37yceWxKG9Y0cnE/yM9D7lso3H/Xbw1Ljtg6fG7XtwOfGlcdtXvOpzYfnOJ04Iy6/e8YbGslXfWBTWnbjzR2E5tAuRDc+3zEcf6SQIE9eVXYhCkNiFKASJXYhCkNiFKASJXYhCkNiFKASJXYhCGHA8u7dKnRzGjbeYVhjAFsV+18d+7rmNZbvPjrtx/OgsPXBcPJLNkj3RvO8TC+PGfV4yfXfihx95IikPbF++7kBY97QFD4Xlj03Ex+zg2uZ9O/akeA6BRTta+MmJpz2vVgj6LTmXwzEjQcpmXdmFKASJXYhCkNiFKASJXYhCkNiFKASJXYhCkNiFKITB+tkdPEjh2yaNbhYLn80rn80TvuBgc/lxt8d2zzuSONKTudeP2hv7dH1ecwP3v3pxWPfICfH85fMPx8at+X9H4voHmsvvffzZYd03PnBJWG77Y1/2us892lg2cveusO5ENi4jyRWQzgvfNoV4D6RbNLOrzWyfmd3esWyFmd1gZjvq9zjBuBBi6HTz83INcO6UZe8HbnT3dcCN9XchxBwmFbu73wTsn7L4fGBz/Xkz8KY+2yWE6DO9/nFY6e57AOr3xj9fZrbBzLaa2dZR4v93QojZY9afErj7Jndf7+7rFxAHLgghZo9exb7XzFYB1O/7+meSEGI26FXs1wEX1p8vBL7SH3OEELNF6mc3s88D5wDHm9ku4IPA5cAXzOxdwL3AW7ramoGN9B7HG8akt4xnz8rnb7ursezYWxKf6kTcto/3HuMPQNCnx6x+cVh134osHj0uX3Rnkud8SfOc+Sf94Q/julGegG4IfOHjbfKj08WYkIm4/chPn+13OGojmDc+Fbu7X9BQ9JqsrhBi7qDhskIUgsQuRCFI7EIUgsQuRCFI7EIUwoCnkrY4TDWZnjeafjcNYU1cLZkrZeLQoaBy8puZuGHaEk1bvPBwEqoZTEPdFQviU2ji7vt6bztJXZymRW7rumvT9kiSIjw4H9Pw2R73S1d2IQpBYheiECR2IQpBYheiECR2IQpBYheiECR2IQphwFNJez7FbkDkC0+nkk786JnvMvN9hoy06+a0z4Lw3MV746nARp44Omk7Ls584eHYiZZ+9HT8QtZ+m223SD3eVfsRkQ8/6BJd2YUoBIldiEKQ2IUoBIldiEKQ2IUoBIldiEKQ2IUohAHHsxP7RrMY4CDdc9sUuK1iiBN/bp4uOonjbxEbvWDvI3HTo4mfPXFVHzp9ZVi++O6dzU1nYx+y8QVJv4TjMpI+t/nZuIxknoBsqunAT99mLEqEruxCFILELkQhSOxCFILELkQhSOxCFILELkQhSOxCFMJg/eyW+LOzecAD32g0dzrk8e4ZreLZs7bb2h6k6WU0jtPPUjJPzI8D2g+viv3JRwX7lvq6s35J/NFtYulb25bNrxCNvbC4z8O050HV9MpuZleb2T4zu71j2UYzu9/MttWv87J2hBDDpZvb+GuAc6dZ/sfufkb9ur6/Zgkh+k0qdne/Cdg/AFuEELNImwd0l5rZrfVt/vKmlcxsg5ltNbOtox7PhyaEmD16FfvHgdOAM4A9wEeaVnT3Te6+3t3XL7BFPW5OCNGWnsTu7nvdfdzdJ4ArgTP7a5YQot/0JHYzW9Xx9c3A7U3rCiHmBqnz2Mw+D5wDHG9mu4APAueY2RlUXr17gHd3tTWP48JTX3bkG83m8U58+Gn8cUA+Z33SQGZbEg8f7nvLHOWebPqxlbG/+s6PvrixbN1/uTnedsu47tAXnpwvqQ+/5biN0I/fZl6HoCgVu7tfMM3iq7J6Qoi5hYbLClEIErsQhSCxC1EIErsQhSCxC1EIAw5xtdAdkk/v22xulnI5c2ekRK6aLDQ3cpWQu+ba9AsLY9+ZJR7L8cVJzua1h8NiPzg3R02m50vb6cEzt2HUfNt00A3oyi5EIUjsQhSCxC5EIUjsQhSCxC5EIUjsQhSCxC5EIQw+ZXNEG194VjcNaUzaj1JNJ6TpoLOUzS3wQ7EffPEDsR/98efGtr1xXTyVwbW3n9FcmPmyW4QdQ8t+jabn7oI0HXUUIpulH+/xXNSVXYhCkNiFKASJXYhCkNiFKASJXYhCkNiFKASJXYhCGKyf3T32Lya+TafZdxmmsQUI6nZDtO3W01gn6X/TMQJBvPz4/ofDusdtfzwsP+Y/xmn+3rYing76Wpr97GlM+Cym2c7jzXtPH94VbcaUhFOqB5vsfYtCiKcTErsQhSCxC1EIErsQhSCxC1EIErsQhSCxC1EI3aRsXgN8CvgpYALY5O5XmNkK4G+AtVRpm9/q7rFTN99YXB74m32iZUrmFnN1p7HLmU83iU/O/PA2EvRLsumFOx8Kyw+Oxts+diT2N/vjLfzJWUx5lto4Sg+e9Gk6V382NqIFrfIEtPSzjwHvdffnA2cBl5jZC4D3Aze6+zrgxvq7EGKOkord3fe4+/fqzweB7cBq4Hxgc73aZuBNs2WkEKI9M/rPbmZrgRcDNwMr3X0PVD8IwLP7bZwQon90LXYzWwp8Cfgdd390BvU2mNlWM9s6ypFebBRC9IGuxG5mC6iE/ll3/3K9eK+ZrarLVwH7pqvr7pvcfb27r1/A3EzyJ0QJpGI3MwOuAra7+0c7iq4DLqw/Xwh8pf/mCSH6RTchrq8A3gncZmbb6mWXAZcDXzCzdwH3Am9JW7LYTZWlNg7JptdNQ2B7J3OtpaGcmduvRYhrtm2OxG6ew0eWhuWLLTlm3tzv81aeEFYd3/tA3HZC1K9p+GziBs7rJ67g6HzMXNA9kord3bfQnE36Nf01RwgxW2gEnRCFILELUQgSuxCFILELUQgSuxCFILELUQgDnko68aW3nJK5DVE4JCTTEo8lU2BnfviWqYnDsQvZlMdJOunlR/8kLD9pfuyHf+WLftBY9uCylWFd3530WzZ2Ijpf0nEZyTHJwpLnt0jZ3DJddBO6sgtRCBK7EIUgsQtRCBK7EIUgsQtRCBK7EIUgsQtRCIP1s0PsS0/8izYv8KuOJLvSNm3yLNWFLqaaTsYfWODSTW1LfNX37zs2LL979FBY/o4Tvt1Y9uFTLwrrLt6RxJS3mf8g8aNnPvx0CoIk3j2aZyAbGxGOCVHKZiGExC5EIUjsQhSCxC5EIUjsQhSCxC5EIUjsQhTC4P3sAWEqWvKY85DMr5qlXZ7F+OPUFx7MvQ5JvyRzkI/fvycsP3bLiWH5+045Pyx/yTH3Nhdm3ZaNjciycEfHLGk78+GnKZ+TsROhLz05ZuG5GpwKurILUQgSuxCFILELUQgSuxCFILELUQgSuxCFILELUQipn93M1gCfAn4KmAA2ufsVZrYRuBiYTKJ9mbtf38aYNvHJqd8zmz89m7s98KXnfvIk+Dkhz/8ezGmfxcon/ubjr/znsPz+h18alt83b11j2fKb7wzrTrTIS5+Szfve9phm5dG4j1b5E5p99N0MqhkD3uvu3zOzZcAtZnZDXfbH7v5HXbQhhBgyqdjdfQ+wp/580My2A6tn2zAhRH+Z0X92M1sLvBi4uV50qZndamZXm9nyhjobzGyrmW0d5UgrY4UQvdO12M1sKfAl4Hfc/VHg48BpwBlUV/6PTFfP3Te5+3p3X7+ARX0wWQjRC12J3cwWUAn9s+7+ZQB33+vu4+4+AVwJnDl7Zgoh2pKK3cwMuArY7u4f7Vi+qmO1NwO39988IUS/6OZp/CuAdwK3mdm2etllwAVmdgZVoOI9wLvTlsziKXST6XdD2rg66MZF1ezSSKcNzsJn26Z0jlwx3iIsGFIX1dIv3hyWh00H5wLQRYruzCXZ+3TNGen5kjbQuzs2tD1wEXfzNH4L0zvvWvnUhRCDRSPohCgEiV2IQpDYhSgEiV2IQpDYhSgEiV2IQhjsVNLuoX8y8oumTWd+z8RfnBGGPLYNxcxsm9ciJXQ2viDzNyfTGme08XVnU4un/R50a9u2U9IxAlHVpM+jtkeb6+rKLkQhSOxCFILELkQhSOxCFILELkQhSOxCFILELkQhmLdMNzyjjZk9AOzsWHQ88ODADJgZc9W2uWoXyLZe6adtJ7v7CdMVDFTsT9m42VZ3Xz80AwLmqm1z1S6Qbb0yKNt0Gy9EIUjsQhTCsMW+acjbj5irts1Vu0C29cpAbBvqf3YhxOAY9pVdCDEgJHYhCmEoYjezc83sh2Z2l5m9fxg2NGFm95jZbWa2zcy2DtmWq81sn5nd3rFshZndYGY76vdpc+wNybaNZnZ/3XfbzOy8Idm2xsy+YWbbzewOM3tPvXyofRfYNZB+G/h/djObB9wJvA7YBXwXuMDd/2WghjRgZvcA69196AMwzOxVwCHgU+7+wnrZ/wb2u/vl9Q/lcnd/3xyxbSNwaNhpvOtsRas604wDbwIuYoh9F9j1VgbQb8O4sp8J3OXuP3L3J4C/Bs4fgh1zHne/Cdg/ZfH5wOb682aqk2XgNNg2J3D3Pe7+vfrzQWAyzfhQ+y6wayAMQ+yrgfs6vu9ibuV7d+DvzewWM9swbGOmYaW774Hq5AGePWR7ppKm8R4kU9KMz5m+6yX9eVuGIfbpJsmaS/6/V7j7zwKvBy6pb1dFd3SVxntQTJNmfE7Qa/rztgxD7LuANR3fTwR2D8GOaXH33fX7PuBa5l4q6r2TGXTr931DtuffmEtpvKdLM84c6Lthpj8fhti/C6wzs1PMbCHwduC6IdjxFMxsSf3gBDNbAvwCcy8V9XXAhfXnC4GvDNGWJzFX0ng3pRlnyH039PTn7j7wF3Ae1RP5fwX+2zBsaLDrVOD79euOYdsGfJ7qtm6U6o7oXcBxwI3Ajvp9xRyy7dPAbcCtVMJaNSTbzqb6a3grsK1+nTfsvgvsGki/abisEIWgEXRCFILELkQhSOxCFILELkQhSOxCFILELkQhSOxCFML/B5F4p21a9OKwAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# train 데이터에 있는 이미지 출력해보기(Index에 있는 이미지, 이미지에 해당하는 숫자와 문자)\n",
    "idx = 30\n",
    "img = train.loc[idx, '0':].values.reshape(28, 28).astype(int)\n",
    "digit = train.loc[idx, 'digit']\n",
    "letter = train.loc[idx, 'letter']\n",
    "\n",
    "plt.title('Index: %i, Digit: %s, Letter: %s'%(idx, digit, letter))\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2048, 784)\n",
      "(2048,)\n",
      "(1372, 784)\n",
      "(676, 784)\n",
      "(1372,)\n",
      "(676,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "# 사용하지 않는 column 정보들은 drop한다. (train, test)\n",
    "x_train2 = train.drop(['id', 'digit', 'letter'], axis=1).values\n",
    "y_train2 = train['digit']\n",
    "print(x_train2.shape)\n",
    "print(y_train2.shape)\n",
    "\n",
    "# x_train과 y_train 값을 train, validation set으로 나눈다.\n",
    "x_train2, x_valid2, y_train2, y_valid2 = train_test_split(x_train2, y_train2, test_size=0.33)\n",
    "print(x_train2.shape)\n",
    "print(x_valid2.shape)\n",
    "print(y_train2.shape)\n",
    "print(y_valid2.shape)\n",
    "\n",
    "# train 이미지를 reshape 해준다.\n",
    "x_train2 = x_train2.reshape(-1, 28, 28, 1)\n",
    "x_valid2 = x_valid2.reshape(-1, 28, 28, 1)\n",
    "\n",
    "# train 이미지를 normalization 해준다.\n",
    "x_train2 = x_train2/255\n",
    "x_valid2 = x_valid2/255\n",
    "\n",
    "# y값들은 one_hot encoding 해준다.\n",
    "y_train2 = to_categorical(y_train2, 10)\n",
    "y_valid2 = to_categorical(y_valid2, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\User\\Anaconda3\\envs\\test\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 28, 28, 16)        160       \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 28, 28, 16)        64        \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 28, 28, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 28, 28, 32)        4640      \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 28, 28, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 28, 28, 32)        25632     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 28, 28, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 28, 28, 32)        25632     \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 28, 28, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 28, 28, 32)        25632     \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 28, 28, 32)        128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 9, 9, 32)          0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 9, 9, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 9, 9, 64)          18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 9, 9, 64)          256       \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 9, 9, 64)          102464    \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 9, 9, 64)          256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 3, 3, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 3, 3, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 576)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 287,274\n",
      "Trainable params: 286,346\n",
      "Non-trainable params: 928\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import *\n",
    "from keras import Sequential\n",
    "\n",
    "model = Sequential()\n",
    "    \n",
    "model.add(Conv2D(16,(3,3),activation='relu',input_shape=(28,28,1),padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.3))\n",
    "    \n",
    "model.add(Conv2D(32,(3,3),activation='relu',padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(32,(5,5),activation='relu',padding='same')) \n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(32,(5,5),activation='relu',padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(32,(5,5),activation='relu',padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D((3,3)))\n",
    "model.add(Dropout(0.3))\n",
    "    \n",
    "model.add(Conv2D(64,(3,3),activation='relu',padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(64,(5,5),activation='relu',padding='same')) \n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D((3,3)))\n",
    "model.add(Dropout(0.3))\n",
    "    \n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(128,activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(64,activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(Dense(10,activation='softmax'))\n",
    "\n",
    "# model.summary()를 통해 모델을 살펴보세요.\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 compile(loss, optimizer 등 설정)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\User\\Anaconda3\\envs\\test\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 1372 samples, validate on 676 samples\n",
      "Epoch 1/200\n",
      "1372/1372 [==============================] - 12s 9ms/step - loss: 2.9748 - accuracy: 0.1334 - val_loss: 2.3721 - val_accuracy: 0.1050\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 2.37211, saving model to D:/DACON_MNIST/data/model.weights.best.hdf5\n",
      "Epoch 2/200\n",
      "1372/1372 [==============================] - 11s 8ms/step - loss: 2.6079 - accuracy: 0.1574 - val_loss: 2.7973 - val_accuracy: 0.1050\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 2.37211\n",
      "Epoch 3/200\n",
      "1372/1372 [==============================] - 11s 8ms/step - loss: 2.3203 - accuracy: 0.2179 - val_loss: 3.4361 - val_accuracy: 0.1050\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 2.37211\n",
      "Epoch 4/200\n",
      "1372/1372 [==============================] - 11s 8ms/step - loss: 2.1061 - accuracy: 0.2901 - val_loss: 4.6995 - val_accuracy: 0.1050\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 2.37211\n",
      "Epoch 5/200\n",
      "1372/1372 [==============================] - 12s 9ms/step - loss: 1.9478 - accuracy: 0.3090 - val_loss: 5.5922 - val_accuracy: 0.1050\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 2.37211\n",
      "Epoch 6/200\n",
      "1372/1372 [==============================] - 11s 8ms/step - loss: 1.7479 - accuracy: 0.3929 - val_loss: 6.4599 - val_accuracy: 0.1050\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 2.37211\n",
      "Epoch 7/200\n",
      "1372/1372 [==============================] - 12s 9ms/step - loss: 1.5824 - accuracy: 0.4461 - val_loss: 7.5514 - val_accuracy: 0.1050\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 2.37211\n",
      "Epoch 8/200\n",
      "1372/1372 [==============================] - 12s 9ms/step - loss: 1.5209 - accuracy: 0.4650 - val_loss: 7.3391 - val_accuracy: 0.1050\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 2.37211\n",
      "Epoch 9/200\n",
      "1372/1372 [==============================] - 12s 8ms/step - loss: 1.3993 - accuracy: 0.5204 - val_loss: 6.1116 - val_accuracy: 0.1050\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 2.37211\n",
      "Epoch 10/200\n",
      "1372/1372 [==============================] - 11s 8ms/step - loss: 1.3074 - accuracy: 0.5474 - val_loss: 5.9694 - val_accuracy: 0.1050\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 2.37211\n",
      "Epoch 11/200\n",
      "1372/1372 [==============================] - 11s 8ms/step - loss: 1.1857 - accuracy: 0.5955 - val_loss: 5.4237 - val_accuracy: 0.1050\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 2.37211\n",
      "Epoch 12/200\n",
      "1372/1372 [==============================] - 11s 8ms/step - loss: 1.0990 - accuracy: 0.6407 - val_loss: 5.6335 - val_accuracy: 0.1050\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 2.37211\n",
      "Epoch 13/200\n",
      "1372/1372 [==============================] - 11s 8ms/step - loss: 1.0067 - accuracy: 0.6603 - val_loss: 4.3008 - val_accuracy: 0.1317\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 2.37211\n",
      "Epoch 14/200\n",
      "1372/1372 [==============================] - 11s 8ms/step - loss: 0.9358 - accuracy: 0.6880 - val_loss: 2.8810 - val_accuracy: 0.2263\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 2.37211\n",
      "Epoch 15/200\n",
      "1372/1372 [==============================] - 11s 8ms/step - loss: 0.9086 - accuracy: 0.6990 - val_loss: 2.6093 - val_accuracy: 0.2944\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 2.37211\n",
      "Epoch 16/200\n",
      "1372/1372 [==============================] - 11s 8ms/step - loss: 0.8537 - accuracy: 0.7252 - val_loss: 1.5919 - val_accuracy: 0.4911\n",
      "\n",
      "Epoch 00016: val_loss improved from 2.37211 to 1.59195, saving model to D:/DACON_MNIST/data/model.weights.best.hdf5\n",
      "Epoch 17/200\n",
      "1372/1372 [==============================] - 11s 8ms/step - loss: 0.7705 - accuracy: 0.7456 - val_loss: 1.5654 - val_accuracy: 0.5237\n",
      "\n",
      "Epoch 00017: val_loss improved from 1.59195 to 1.56537, saving model to D:/DACON_MNIST/data/model.weights.best.hdf5\n",
      "Epoch 18/200\n",
      "1372/1372 [==============================] - 11s 8ms/step - loss: 0.7408 - accuracy: 0.7595 - val_loss: 1.4411 - val_accuracy: 0.5518\n",
      "\n",
      "Epoch 00018: val_loss improved from 1.56537 to 1.44108, saving model to D:/DACON_MNIST/data/model.weights.best.hdf5\n",
      "Epoch 19/200\n",
      "1372/1372 [==============================] - 11s 8ms/step - loss: 0.6937 - accuracy: 0.7741 - val_loss: 1.1331 - val_accuracy: 0.6317\n",
      "\n",
      "Epoch 00019: val_loss improved from 1.44108 to 1.13309, saving model to D:/DACON_MNIST/data/model.weights.best.hdf5\n",
      "Epoch 20/200\n",
      "1372/1372 [==============================] - 11s 8ms/step - loss: 0.7015 - accuracy: 0.7733 - val_loss: 0.7827 - val_accuracy: 0.7337\n",
      "\n",
      "Epoch 00020: val_loss improved from 1.13309 to 0.78269, saving model to D:/DACON_MNIST/data/model.weights.best.hdf5\n",
      "Epoch 21/200\n",
      "1372/1372 [==============================] - 11s 8ms/step - loss: 0.6449 - accuracy: 0.7821 - val_loss: 0.8253 - val_accuracy: 0.7367\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.78269\n",
      "Epoch 22/200\n",
      "1372/1372 [==============================] - 11s 8ms/step - loss: 0.6257 - accuracy: 0.7974 - val_loss: 0.8052 - val_accuracy: 0.7234\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.78269\n",
      "Epoch 23/200\n",
      "1372/1372 [==============================] - 11s 8ms/step - loss: 0.5607 - accuracy: 0.8163 - val_loss: 0.8299 - val_accuracy: 0.7411\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.78269\n",
      "Epoch 24/200\n",
      "1372/1372 [==============================] - 11s 8ms/step - loss: 0.5509 - accuracy: 0.8214 - val_loss: 0.9831 - val_accuracy: 0.6760\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.78269\n",
      "Epoch 25/200\n",
      "1372/1372 [==============================] - 11s 8ms/step - loss: 0.5312 - accuracy: 0.8236 - val_loss: 0.6488 - val_accuracy: 0.7885\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.78269 to 0.64878, saving model to D:/DACON_MNIST/data/model.weights.best.hdf5\n",
      "Epoch 26/200\n",
      "1372/1372 [==============================] - 11s 8ms/step - loss: 0.5498 - accuracy: 0.8171 - val_loss: 1.0131 - val_accuracy: 0.6479\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.64878\n",
      "Epoch 27/200\n",
      "1372/1372 [==============================] - 11s 8ms/step - loss: 0.4843 - accuracy: 0.8448 - val_loss: 0.6823 - val_accuracy: 0.7633\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.64878\n",
      "Epoch 28/200\n",
      "1372/1372 [==============================] - 11s 8ms/step - loss: 0.4492 - accuracy: 0.8520 - val_loss: 0.8914 - val_accuracy: 0.7071\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.64878\n",
      "Epoch 29/200\n",
      "1372/1372 [==============================] - 11s 8ms/step - loss: 0.4486 - accuracy: 0.8520 - val_loss: 0.6406 - val_accuracy: 0.7840\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.64878 to 0.64063, saving model to D:/DACON_MNIST/data/model.weights.best.hdf5\n",
      "Epoch 30/200\n",
      "1372/1372 [==============================] - 11s 8ms/step - loss: 0.4645 - accuracy: 0.8484 - val_loss: 0.6496 - val_accuracy: 0.7751\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.64063\n",
      "Epoch 31/200\n",
      "1372/1372 [==============================] - 11s 8ms/step - loss: 0.3996 - accuracy: 0.8724 - val_loss: 0.6890 - val_accuracy: 0.7633\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.64063\n",
      "Epoch 32/200\n",
      "1372/1372 [==============================] - 12s 8ms/step - loss: 0.3926 - accuracy: 0.8739 - val_loss: 0.6398 - val_accuracy: 0.7855\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.64063 to 0.63979, saving model to D:/DACON_MNIST/data/model.weights.best.hdf5\n",
      "Epoch 33/200\n",
      "1372/1372 [==============================] - 11s 8ms/step - loss: 0.3506 - accuracy: 0.8812 - val_loss: 0.7488 - val_accuracy: 0.7500\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.63979\n",
      "Epoch 34/200\n",
      "1372/1372 [==============================] - 12s 8ms/step - loss: 0.3561 - accuracy: 0.8863 - val_loss: 0.7709 - val_accuracy: 0.7337\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.63979\n",
      "Epoch 35/200\n",
      "1372/1372 [==============================] - 11s 8ms/step - loss: 0.3478 - accuracy: 0.8950 - val_loss: 0.7560 - val_accuracy: 0.7426\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.63979\n",
      "Epoch 36/200\n",
      "1372/1372 [==============================] - 11s 8ms/step - loss: 0.3063 - accuracy: 0.8914 - val_loss: 0.5591 - val_accuracy: 0.8195\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.63979 to 0.55912, saving model to D:/DACON_MNIST/data/model.weights.best.hdf5\n",
      "Epoch 37/200\n",
      "1372/1372 [==============================] - 11s 8ms/step - loss: 0.2858 - accuracy: 0.9074 - val_loss: 0.8363 - val_accuracy: 0.7263\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00037: val_loss did not improve from 0.55912\n",
      "Epoch 38/200\n",
      "1372/1372 [==============================] - 11s 8ms/step - loss: 0.3056 - accuracy: 0.8972 - val_loss: 0.5413 - val_accuracy: 0.8195\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.55912 to 0.54125, saving model to D:/DACON_MNIST/data/model.weights.best.hdf5\n",
      "Epoch 39/200\n",
      "1372/1372 [==============================] - 11s 8ms/step - loss: 0.2773 - accuracy: 0.9191 - val_loss: 0.5995 - val_accuracy: 0.7944\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.54125\n",
      "Epoch 40/200\n",
      "1372/1372 [==============================] - 12s 9ms/step - loss: 0.2600 - accuracy: 0.9162 - val_loss: 0.7367 - val_accuracy: 0.7589\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.54125\n",
      "Epoch 41/200\n",
      "1372/1372 [==============================] - 11s 8ms/step - loss: 0.2683 - accuracy: 0.9038 - val_loss: 0.5656 - val_accuracy: 0.8033\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.54125\n",
      "Epoch 42/200\n",
      "1372/1372 [==============================] - 11s 8ms/step - loss: 0.2460 - accuracy: 0.9191 - val_loss: 0.5999 - val_accuracy: 0.8121\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.54125\n",
      "Epoch 43/200\n",
      "1372/1372 [==============================] - 11s 8ms/step - loss: 0.2644 - accuracy: 0.9213 - val_loss: 0.6671 - val_accuracy: 0.7766\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.54125\n",
      "Epoch 44/200\n",
      "1372/1372 [==============================] - 11s 8ms/step - loss: 0.2594 - accuracy: 0.9133 - val_loss: 0.7076 - val_accuracy: 0.7870\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.54125\n",
      "Epoch 45/200\n",
      "1372/1372 [==============================] - 11s 8ms/step - loss: 0.2309 - accuracy: 0.9315 - val_loss: 0.5660 - val_accuracy: 0.8077\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.54125\n",
      "Epoch 46/200\n",
      "1372/1372 [==============================] - 11s 8ms/step - loss: 0.1978 - accuracy: 0.9388 - val_loss: 0.5858 - val_accuracy: 0.8240\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.54125\n",
      "Epoch 47/200\n",
      "1372/1372 [==============================] - 11s 8ms/step - loss: 0.1871 - accuracy: 0.9337 - val_loss: 0.7125 - val_accuracy: 0.7737\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.54125\n",
      "Epoch 48/200\n",
      "1372/1372 [==============================] - 11s 8ms/step - loss: 0.2039 - accuracy: 0.9344 - val_loss: 0.5853 - val_accuracy: 0.8033\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.54125\n",
      "Epoch 49/200\n",
      "1372/1372 [==============================] - 11s 8ms/step - loss: 0.2288 - accuracy: 0.9257 - val_loss: 0.6891 - val_accuracy: 0.7855\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.54125\n",
      "Epoch 50/200\n",
      "1372/1372 [==============================] - 11s 8ms/step - loss: 0.1943 - accuracy: 0.9337 - val_loss: 0.5047 - val_accuracy: 0.8417\n",
      "\n",
      "Epoch 00050: val_loss improved from 0.54125 to 0.50469, saving model to D:/DACON_MNIST/data/model.weights.best.hdf5\n",
      "Epoch 51/200\n",
      "1372/1372 [==============================] - 11s 8ms/step - loss: 0.1713 - accuracy: 0.9497 - val_loss: 0.5351 - val_accuracy: 0.8388\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.50469\n",
      "Epoch 52/200\n",
      "1372/1372 [==============================] - 11s 8ms/step - loss: 0.1536 - accuracy: 0.9541 - val_loss: 0.5212 - val_accuracy: 0.8447\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.50469\n",
      "Epoch 53/200\n",
      "1372/1372 [==============================] - 11s 8ms/step - loss: 0.1767 - accuracy: 0.9526 - val_loss: 0.4959 - val_accuracy: 0.8491\n",
      "\n",
      "Epoch 00053: val_loss improved from 0.50469 to 0.49590, saving model to D:/DACON_MNIST/data/model.weights.best.hdf5\n",
      "Epoch 54/200\n",
      "1372/1372 [==============================] - 11s 8ms/step - loss: 0.1396 - accuracy: 0.9548 - val_loss: 0.5729 - val_accuracy: 0.8240\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.49590\n",
      "Epoch 55/200\n",
      "1372/1372 [==============================] - 11s 8ms/step - loss: 0.1448 - accuracy: 0.9570 - val_loss: 0.5709 - val_accuracy: 0.8358\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.49590\n",
      "Epoch 56/200\n",
      "1372/1372 [==============================] - 11s 8ms/step - loss: 0.1422 - accuracy: 0.9526 - val_loss: 0.5970 - val_accuracy: 0.8343\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.49590\n",
      "Epoch 57/200\n",
      "1372/1372 [==============================] - 11s 8ms/step - loss: 0.1549 - accuracy: 0.9548 - val_loss: 0.6354 - val_accuracy: 0.8121\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.49590\n",
      "Epoch 58/200\n",
      "1372/1372 [==============================] - 11s 8ms/step - loss: 0.1663 - accuracy: 0.9490 - val_loss: 0.5144 - val_accuracy: 0.8550\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.49590\n",
      "Epoch 59/200\n",
      "1372/1372 [==============================] - 11s 8ms/step - loss: 0.1420 - accuracy: 0.9555 - val_loss: 0.4821 - val_accuracy: 0.8476\n",
      "\n",
      "Epoch 00059: val_loss improved from 0.49590 to 0.48207, saving model to D:/DACON_MNIST/data/model.weights.best.hdf5\n",
      "Epoch 60/200\n",
      "1372/1372 [==============================] - 11s 8ms/step - loss: 0.1329 - accuracy: 0.9592 - val_loss: 0.4707 - val_accuracy: 0.8698\n",
      "\n",
      "Epoch 00060: val_loss improved from 0.48207 to 0.47066, saving model to D:/DACON_MNIST/data/model.weights.best.hdf5\n",
      "Epoch 61/200\n",
      "1372/1372 [==============================] - 11s 8ms/step - loss: 0.1418 - accuracy: 0.9548 - val_loss: 0.5824 - val_accuracy: 0.8314\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.47066\n",
      "Epoch 62/200\n",
      "1372/1372 [==============================] - 11s 8ms/step - loss: 0.1595 - accuracy: 0.9519 - val_loss: 0.4985 - val_accuracy: 0.8462\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.47066\n",
      "Epoch 63/200\n",
      "1372/1372 [==============================] - 11s 8ms/step - loss: 0.1299 - accuracy: 0.9606 - val_loss: 0.5261 - val_accuracy: 0.8358\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.47066\n",
      "Epoch 64/200\n",
      "1372/1372 [==============================] - 11s 8ms/step - loss: 0.1400 - accuracy: 0.9490 - val_loss: 0.4979 - val_accuracy: 0.8565\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.47066\n",
      "Epoch 65/200\n",
      "1372/1372 [==============================] - 11s 8ms/step - loss: 0.1257 - accuracy: 0.9628 - val_loss: 0.5764 - val_accuracy: 0.8284\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.47066\n",
      "Epoch 66/200\n",
      "1372/1372 [==============================] - 11s 8ms/step - loss: 0.1078 - accuracy: 0.9679 - val_loss: 0.5492 - val_accuracy: 0.8550\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.47066\n",
      "Epoch 67/200\n",
      "1372/1372 [==============================] - 11s 8ms/step - loss: 0.1073 - accuracy: 0.9665 - val_loss: 0.5735 - val_accuracy: 0.8388\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.47066\n",
      "Epoch 68/200\n",
      "1372/1372 [==============================] - 11s 8ms/step - loss: 0.1107 - accuracy: 0.9687 - val_loss: 0.4926 - val_accuracy: 0.8580\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.47066\n",
      "Epoch 69/200\n",
      "1372/1372 [==============================] - 11s 8ms/step - loss: 0.1053 - accuracy: 0.9657 - val_loss: 0.4954 - val_accuracy: 0.8624\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.47066\n",
      "Epoch 70/200\n",
      "1372/1372 [==============================] - 11s 8ms/step - loss: 0.0945 - accuracy: 0.9679 - val_loss: 0.5227 - val_accuracy: 0.8624\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.47066\n",
      "Epoch 71/200\n",
      "1372/1372 [==============================] - 11s 8ms/step - loss: 0.0954 - accuracy: 0.9708 - val_loss: 0.5335 - val_accuracy: 0.8417\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.47066\n",
      "Epoch 72/200\n",
      "1372/1372 [==============================] - 11s 8ms/step - loss: 0.0932 - accuracy: 0.9679 - val_loss: 0.5313 - val_accuracy: 0.8713\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.47066\n",
      "Epoch 73/200\n",
      "1372/1372 [==============================] - 11s 8ms/step - loss: 0.0835 - accuracy: 0.9738 - val_loss: 0.5934 - val_accuracy: 0.8240\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.47066\n",
      "Epoch 74/200\n",
      "1372/1372 [==============================] - 11s 8ms/step - loss: 0.0877 - accuracy: 0.9730 - val_loss: 0.5455 - val_accuracy: 0.8521\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.47066\n",
      "Epoch 75/200\n",
      "1372/1372 [==============================] - 11s 8ms/step - loss: 0.0941 - accuracy: 0.9701 - val_loss: 0.5129 - val_accuracy: 0.8609\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.47066\n",
      "Epoch 76/200\n",
      "1372/1372 [==============================] - 11s 8ms/step - loss: 0.0992 - accuracy: 0.9679 - val_loss: 0.6232 - val_accuracy: 0.8254\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.47066\n",
      "Epoch 77/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1372/1372 [==============================] - 11s 8ms/step - loss: 0.0920 - accuracy: 0.9738 - val_loss: 0.5874 - val_accuracy: 0.8565\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.47066\n",
      "Epoch 78/200\n",
      "1372/1372 [==============================] - 11s 8ms/step - loss: 0.1050 - accuracy: 0.9665 - val_loss: 0.4973 - val_accuracy: 0.8609\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.47066\n",
      "Epoch 79/200\n",
      "1372/1372 [==============================] - 11s 8ms/step - loss: 0.1282 - accuracy: 0.9599 - val_loss: 0.6030 - val_accuracy: 0.8462\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.47066\n",
      "Epoch 80/200\n",
      "1372/1372 [==============================] - 11s 8ms/step - loss: 0.1168 - accuracy: 0.9614 - val_loss: 0.5273 - val_accuracy: 0.8565\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.47066\n",
      "Epoch 81/200\n",
      "1372/1372 [==============================] - 12s 8ms/step - loss: 0.0947 - accuracy: 0.9687 - val_loss: 0.5528 - val_accuracy: 0.8506\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.47066\n",
      "Epoch 82/200\n",
      "1372/1372 [==============================] - 11s 8ms/step - loss: 0.0949 - accuracy: 0.9687 - val_loss: 0.6084 - val_accuracy: 0.8521\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.47066\n",
      "Epoch 83/200\n",
      "1372/1372 [==============================] - 12s 9ms/step - loss: 0.1249 - accuracy: 0.9570 - val_loss: 0.5890 - val_accuracy: 0.8565\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.47066\n",
      "Epoch 84/200\n",
      "1372/1372 [==============================] - 17s 12ms/step - loss: 0.0880 - accuracy: 0.9701 - val_loss: 0.6181 - val_accuracy: 0.8521\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.47066\n",
      "Epoch 85/200\n",
      "1372/1372 [==============================] - 20s 15ms/step - loss: 0.0796 - accuracy: 0.9759 - val_loss: 0.4848 - val_accuracy: 0.8654\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.47066\n",
      "Epoch 86/200\n",
      "1372/1372 [==============================] - 21s 16ms/step - loss: 0.0780 - accuracy: 0.9774 - val_loss: 0.4699 - val_accuracy: 0.8728\n",
      "\n",
      "Epoch 00086: val_loss improved from 0.47066 to 0.46989, saving model to D:/DACON_MNIST/data/model.weights.best.hdf5\n",
      "Epoch 87/200\n",
      "1372/1372 [==============================] - 21s 15ms/step - loss: 0.0640 - accuracy: 0.9781 - val_loss: 0.4781 - val_accuracy: 0.8743\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.46989\n",
      "Epoch 88/200\n",
      "1372/1372 [==============================] - 19s 14ms/step - loss: 0.0780 - accuracy: 0.9767 - val_loss: 0.5244 - val_accuracy: 0.8639\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.46989\n",
      "Epoch 89/200\n",
      "1372/1372 [==============================] - 11s 8ms/step - loss: 0.0651 - accuracy: 0.9803 - val_loss: 0.5234 - val_accuracy: 0.8743\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.46989\n",
      "Epoch 90/200\n",
      "1372/1372 [==============================] - 11s 8ms/step - loss: 0.0747 - accuracy: 0.9745 - val_loss: 0.5425 - val_accuracy: 0.8669\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.46989\n",
      "Epoch 91/200\n",
      "1372/1372 [==============================] - 11s 8ms/step - loss: 0.0843 - accuracy: 0.9716 - val_loss: 0.6305 - val_accuracy: 0.8536\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.46989\n",
      "Epoch 92/200\n",
      "1372/1372 [==============================] - 11s 8ms/step - loss: 0.0908 - accuracy: 0.9665 - val_loss: 0.5500 - val_accuracy: 0.8476\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.46989\n",
      "Epoch 93/200\n",
      "1372/1372 [==============================] - 12s 9ms/step - loss: 0.0805 - accuracy: 0.9730 - val_loss: 0.5520 - val_accuracy: 0.8580\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.46989\n",
      "Epoch 94/200\n",
      "1372/1372 [==============================] - 12s 8ms/step - loss: 0.1089 - accuracy: 0.9650 - val_loss: 0.5356 - val_accuracy: 0.8654\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.46989\n",
      "Epoch 95/200\n",
      "1372/1372 [==============================] - 11s 8ms/step - loss: 0.0815 - accuracy: 0.9716 - val_loss: 0.5295 - val_accuracy: 0.8743\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.46989\n",
      "Epoch 96/200\n",
      "1372/1372 [==============================] - 13s 9ms/step - loss: 0.0747 - accuracy: 0.9767 - val_loss: 0.4986 - val_accuracy: 0.8595\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.46989\n",
      "Epoch 97/200\n",
      "1372/1372 [==============================] - 12s 9ms/step - loss: 0.1020 - accuracy: 0.9679 - val_loss: 0.5188 - val_accuracy: 0.8639\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.46989\n",
      "Epoch 98/200\n",
      "1372/1372 [==============================] - 12s 9ms/step - loss: 0.0602 - accuracy: 0.9825 - val_loss: 0.5659 - val_accuracy: 0.8580\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.46989\n",
      "Epoch 99/200\n",
      "1372/1372 [==============================] - 12s 9ms/step - loss: 0.0827 - accuracy: 0.9708 - val_loss: 0.5146 - val_accuracy: 0.8609\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.46989\n",
      "Epoch 100/200\n",
      "1372/1372 [==============================] - 11s 8ms/step - loss: 0.0728 - accuracy: 0.9774 - val_loss: 0.5109 - val_accuracy: 0.8728\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.46989\n",
      "Epoch 101/200\n",
      "1372/1372 [==============================] - 11s 8ms/step - loss: 0.0588 - accuracy: 0.9810 - val_loss: 0.5216 - val_accuracy: 0.8595\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 0.46989\n",
      "Epoch 102/200\n",
      "1372/1372 [==============================] - 12s 8ms/step - loss: 0.0594 - accuracy: 0.9818 - val_loss: 0.6824 - val_accuracy: 0.8328\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 0.46989\n",
      "Epoch 103/200\n",
      "1372/1372 [==============================] - 11s 8ms/step - loss: 0.0787 - accuracy: 0.9789 - val_loss: 0.5441 - val_accuracy: 0.8669\n",
      "\n",
      "Epoch 00103: val_loss did not improve from 0.46989\n",
      "Epoch 104/200\n",
      "1372/1372 [==============================] - 11s 8ms/step - loss: 0.0709 - accuracy: 0.9730 - val_loss: 0.5689 - val_accuracy: 0.8669\n",
      "\n",
      "Epoch 00104: val_loss did not improve from 0.46989\n",
      "Epoch 105/200\n",
      "1372/1372 [==============================] - 11s 8ms/step - loss: 0.0666 - accuracy: 0.9781 - val_loss: 0.5973 - val_accuracy: 0.8550\n",
      "\n",
      "Epoch 00105: val_loss did not improve from 0.46989\n",
      "Epoch 106/200\n",
      "1372/1372 [==============================] - 11s 8ms/step - loss: 0.0611 - accuracy: 0.9774 - val_loss: 0.6045 - val_accuracy: 0.8565\n",
      "\n",
      "Epoch 00106: val_loss did not improve from 0.46989\n",
      "Epoch 107/200\n",
      "1372/1372 [==============================] - 11s 8ms/step - loss: 0.0854 - accuracy: 0.9723 - val_loss: 0.6946 - val_accuracy: 0.8299\n",
      "\n",
      "Epoch 00107: val_loss did not improve from 0.46989\n",
      "Epoch 108/200\n",
      "1372/1372 [==============================] - 11s 8ms/step - loss: 0.0504 - accuracy: 0.9832 - val_loss: 0.6137 - val_accuracy: 0.8595\n",
      "\n",
      "Epoch 00108: val_loss did not improve from 0.46989\n",
      "Epoch 109/200\n",
      "1372/1372 [==============================] - 17s 12ms/step - loss: 0.0723 - accuracy: 0.9767 - val_loss: 0.6038 - val_accuracy: 0.8462\n",
      "\n",
      "Epoch 00109: val_loss did not improve from 0.46989\n",
      "Epoch 110/200\n",
      "1372/1372 [==============================] - 15s 11ms/step - loss: 0.0571 - accuracy: 0.9796 - val_loss: 0.5093 - val_accuracy: 0.8595\n",
      "\n",
      "Epoch 00110: val_loss did not improve from 0.46989\n",
      "Epoch 111/200\n",
      "1372/1372 [==============================] - 11s 8ms/step - loss: 0.0453 - accuracy: 0.9854 - val_loss: 0.4570 - val_accuracy: 0.8831\n",
      "\n",
      "Epoch 00111: val_loss improved from 0.46989 to 0.45702, saving model to D:/DACON_MNIST/data/model.weights.best.hdf5\n",
      "Epoch 112/200\n",
      "1372/1372 [==============================] - 11s 8ms/step - loss: 0.0569 - accuracy: 0.9818 - val_loss: 0.4896 - val_accuracy: 0.8698\n",
      "\n",
      "Epoch 00112: val_loss did not improve from 0.45702\n",
      "Epoch 113/200\n",
      "1372/1372 [==============================] - 11s 8ms/step - loss: 0.0606 - accuracy: 0.9810 - val_loss: 0.5708 - val_accuracy: 0.8536\n",
      "\n",
      "Epoch 00113: val_loss did not improve from 0.45702\n",
      "Epoch 114/200\n",
      "1372/1372 [==============================] - 11s 8ms/step - loss: 0.0698 - accuracy: 0.9803 - val_loss: 0.5047 - val_accuracy: 0.8713\n",
      "\n",
      "Epoch 00114: val_loss did not improve from 0.45702\n",
      "Epoch 115/200\n",
      "1372/1372 [==============================] - 11s 8ms/step - loss: 0.0640 - accuracy: 0.9810 - val_loss: 0.6050 - val_accuracy: 0.8402\n",
      "\n",
      "Epoch 00115: val_loss did not improve from 0.45702\n",
      "Epoch 116/200\n",
      "1372/1372 [==============================] - 13s 10ms/step - loss: 0.0725 - accuracy: 0.9752 - val_loss: 0.5250 - val_accuracy: 0.8683\n",
      "\n",
      "Epoch 00116: val_loss did not improve from 0.45702\n",
      "Epoch 117/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1372/1372 [==============================] - 18s 13ms/step - loss: 0.0691 - accuracy: 0.9818 - val_loss: 0.5914 - val_accuracy: 0.8609\n",
      "\n",
      "Epoch 00117: val_loss did not improve from 0.45702\n",
      "Epoch 118/200\n",
      "1372/1372 [==============================] - 11s 8ms/step - loss: 0.0535 - accuracy: 0.9840 - val_loss: 0.5127 - val_accuracy: 0.8817\n",
      "\n",
      "Epoch 00118: val_loss did not improve from 0.45702\n",
      "Epoch 119/200\n",
      "1372/1372 [==============================] - 11s 8ms/step - loss: 0.0577 - accuracy: 0.9789 - val_loss: 0.4919 - val_accuracy: 0.8698\n",
      "\n",
      "Epoch 00119: val_loss did not improve from 0.45702\n",
      "Epoch 120/200\n",
      "1372/1372 [==============================] - 11s 8ms/step - loss: 0.0651 - accuracy: 0.9774 - val_loss: 0.5101 - val_accuracy: 0.8654\n",
      "\n",
      "Epoch 00120: val_loss did not improve from 0.45702\n",
      "Epoch 121/200\n",
      "1372/1372 [==============================] - 11s 8ms/step - loss: 0.0552 - accuracy: 0.9832 - val_loss: 0.6487 - val_accuracy: 0.8624\n",
      "\n",
      "Epoch 00121: val_loss did not improve from 0.45702\n",
      "Epoch 122/200\n",
      "1372/1372 [==============================] - 11s 8ms/step - loss: 0.0419 - accuracy: 0.9818 - val_loss: 0.5926 - val_accuracy: 0.8624\n",
      "\n",
      "Epoch 00122: val_loss did not improve from 0.45702\n",
      "Epoch 123/200\n",
      "1372/1372 [==============================] - 12s 9ms/step - loss: 0.0483 - accuracy: 0.9862 - val_loss: 0.5700 - val_accuracy: 0.8654\n",
      "\n",
      "Epoch 00123: val_loss did not improve from 0.45702\n",
      "Epoch 124/200\n",
      "1372/1372 [==============================] - 11s 8ms/step - loss: 0.0412 - accuracy: 0.9883 - val_loss: 0.5630 - val_accuracy: 0.8698\n",
      "\n",
      "Epoch 00124: val_loss did not improve from 0.45702\n",
      "Epoch 125/200\n",
      "1372/1372 [==============================] - 11s 8ms/step - loss: 0.0589 - accuracy: 0.9840 - val_loss: 0.5129 - val_accuracy: 0.8772\n",
      "\n",
      "Epoch 00125: val_loss did not improve from 0.45702\n",
      "Epoch 126/200\n",
      "1372/1372 [==============================] - 11s 8ms/step - loss: 0.0632 - accuracy: 0.9796 - val_loss: 0.6477 - val_accuracy: 0.8462\n",
      "\n",
      "Epoch 00126: val_loss did not improve from 0.45702\n",
      "Epoch 127/200\n",
      "1372/1372 [==============================] - 11s 8ms/step - loss: 0.0473 - accuracy: 0.9862 - val_loss: 0.5151 - val_accuracy: 0.8802\n",
      "\n",
      "Epoch 00127: val_loss did not improve from 0.45702\n",
      "Epoch 128/200\n",
      "1372/1372 [==============================] - 11s 8ms/step - loss: 0.0404 - accuracy: 0.9876 - val_loss: 0.5452 - val_accuracy: 0.8669\n",
      "\n",
      "Epoch 00128: val_loss did not improve from 0.45702\n",
      "Epoch 129/200\n",
      "1372/1372 [==============================] - 11s 8ms/step - loss: 0.0375 - accuracy: 0.9898 - val_loss: 0.5668 - val_accuracy: 0.8698\n",
      "\n",
      "Epoch 00129: val_loss did not improve from 0.45702\n",
      "Epoch 130/200\n",
      "1372/1372 [==============================] - 11s 8ms/step - loss: 0.0508 - accuracy: 0.9832 - val_loss: 0.6150 - val_accuracy: 0.8506\n",
      "\n",
      "Epoch 00130: val_loss did not improve from 0.45702\n",
      "Epoch 131/200\n",
      "1372/1372 [==============================] - 11s 8ms/step - loss: 0.0531 - accuracy: 0.9832 - val_loss: 0.5311 - val_accuracy: 0.8802\n",
      "\n",
      "Epoch 00131: val_loss did not improve from 0.45702\n",
      "Epoch 132/200\n",
      "1372/1372 [==============================] - 11s 8ms/step - loss: 0.0418 - accuracy: 0.9883 - val_loss: 0.5797 - val_accuracy: 0.8713\n",
      "\n",
      "Epoch 00132: val_loss did not improve from 0.45702\n",
      "Epoch 133/200\n",
      "1372/1372 [==============================] - 11s 8ms/step - loss: 0.0514 - accuracy: 0.9840 - val_loss: 0.5729 - val_accuracy: 0.8772\n",
      "\n",
      "Epoch 00133: val_loss did not improve from 0.45702\n",
      "Epoch 134/200\n",
      "1372/1372 [==============================] - 11s 8ms/step - loss: 0.0386 - accuracy: 0.9847 - val_loss: 0.5982 - val_accuracy: 0.8654\n",
      "\n",
      "Epoch 00134: val_loss did not improve from 0.45702\n",
      "Epoch 135/200\n",
      "1372/1372 [==============================] - 11s 8ms/step - loss: 0.0492 - accuracy: 0.9862 - val_loss: 0.6034 - val_accuracy: 0.8683\n",
      "\n",
      "Epoch 00135: val_loss did not improve from 0.45702\n",
      "Epoch 136/200\n",
      "1372/1372 [==============================] - 11s 8ms/step - loss: 0.0844 - accuracy: 0.9738 - val_loss: 0.6494 - val_accuracy: 0.8609\n",
      "\n",
      "Epoch 00136: val_loss did not improve from 0.45702\n",
      "Epoch 137/200\n",
      "1372/1372 [==============================] - 11s 8ms/step - loss: 0.0516 - accuracy: 0.9832 - val_loss: 0.5452 - val_accuracy: 0.8639\n",
      "\n",
      "Epoch 00137: val_loss did not improve from 0.45702\n",
      "Epoch 138/200\n",
      "1372/1372 [==============================] - 11s 8ms/step - loss: 0.0508 - accuracy: 0.9818 - val_loss: 0.6077 - val_accuracy: 0.8683\n",
      "\n",
      "Epoch 00138: val_loss did not improve from 0.45702\n",
      "Epoch 139/200\n",
      "1372/1372 [==============================] - 11s 8ms/step - loss: 0.0505 - accuracy: 0.9862 - val_loss: 0.5912 - val_accuracy: 0.8713\n",
      "\n",
      "Epoch 00139: val_loss did not improve from 0.45702\n",
      "Epoch 140/200\n",
      "1372/1372 [==============================] - 11s 8ms/step - loss: 0.0692 - accuracy: 0.9759 - val_loss: 0.5804 - val_accuracy: 0.8683\n",
      "\n",
      "Epoch 00140: val_loss did not improve from 0.45702\n",
      "Epoch 141/200\n",
      "1372/1372 [==============================] - 11s 8ms/step - loss: 0.0704 - accuracy: 0.9752 - val_loss: 0.5258 - val_accuracy: 0.8846\n",
      "\n",
      "Epoch 00141: val_loss did not improve from 0.45702\n",
      "Epoch 142/200\n",
      "1372/1372 [==============================] - 11s 8ms/step - loss: 0.0614 - accuracy: 0.9796 - val_loss: 0.5708 - val_accuracy: 0.8743\n",
      "\n",
      "Epoch 00142: val_loss did not improve from 0.45702\n",
      "Epoch 143/200\n",
      "1372/1372 [==============================] - 11s 8ms/step - loss: 0.0571 - accuracy: 0.9832 - val_loss: 0.5480 - val_accuracy: 0.8743\n",
      "\n",
      "Epoch 00143: val_loss did not improve from 0.45702\n",
      "Epoch 144/200\n",
      "1372/1372 [==============================] - 11s 8ms/step - loss: 0.0444 - accuracy: 0.9840 - val_loss: 0.5374 - val_accuracy: 0.8772\n",
      "\n",
      "Epoch 00144: val_loss did not improve from 0.45702\n",
      "Epoch 145/200\n",
      "1372/1372 [==============================] - 11s 8ms/step - loss: 0.0301 - accuracy: 0.9883 - val_loss: 0.5442 - val_accuracy: 0.8802\n",
      "\n",
      "Epoch 00145: val_loss did not improve from 0.45702\n",
      "Epoch 146/200\n",
      "1372/1372 [==============================] - 11s 8ms/step - loss: 0.0363 - accuracy: 0.9883 - val_loss: 0.5598 - val_accuracy: 0.8817\n",
      "\n",
      "Epoch 00146: val_loss did not improve from 0.45702\n",
      "Epoch 147/200\n",
      "1372/1372 [==============================] - 11s 8ms/step - loss: 0.0293 - accuracy: 0.9913 - val_loss: 0.5118 - val_accuracy: 0.8831\n",
      "\n",
      "Epoch 00147: val_loss did not improve from 0.45702\n",
      "Epoch 148/200\n",
      "1372/1372 [==============================] - 11s 8ms/step - loss: 0.0270 - accuracy: 0.9905 - val_loss: 0.5979 - val_accuracy: 0.8787\n",
      "\n",
      "Epoch 00148: val_loss did not improve from 0.45702\n",
      "Epoch 149/200\n",
      "1372/1372 [==============================] - 11s 8ms/step - loss: 0.0432 - accuracy: 0.9869 - val_loss: 0.4851 - val_accuracy: 0.8817\n",
      "\n",
      "Epoch 00149: val_loss did not improve from 0.45702\n",
      "Epoch 150/200\n",
      "1372/1372 [==============================] - 11s 8ms/step - loss: 0.0556 - accuracy: 0.9854 - val_loss: 0.5557 - val_accuracy: 0.8743\n",
      "\n",
      "Epoch 00150: val_loss did not improve from 0.45702\n",
      "Epoch 151/200\n",
      "1372/1372 [==============================] - 11s 8ms/step - loss: 0.0523 - accuracy: 0.9810 - val_loss: 0.5436 - val_accuracy: 0.8713\n",
      "\n",
      "Epoch 00151: val_loss did not improve from 0.45702\n",
      "Epoch 152/200\n",
      "1372/1372 [==============================] - 11s 8ms/step - loss: 0.0503 - accuracy: 0.9854 - val_loss: 0.7155 - val_accuracy: 0.8462\n",
      "\n",
      "Epoch 00152: val_loss did not improve from 0.45702\n",
      "Epoch 153/200\n",
      "1372/1372 [==============================] - 11s 8ms/step - loss: 0.0556 - accuracy: 0.9810 - val_loss: 0.5621 - val_accuracy: 0.8757\n",
      "\n",
      "Epoch 00153: val_loss did not improve from 0.45702\n",
      "Epoch 154/200\n",
      "1372/1372 [==============================] - 11s 8ms/step - loss: 0.0434 - accuracy: 0.9869 - val_loss: 0.6995 - val_accuracy: 0.8521\n",
      "\n",
      "Epoch 00154: val_loss did not improve from 0.45702\n",
      "Epoch 155/200\n",
      "1372/1372 [==============================] - 11s 8ms/step - loss: 0.0429 - accuracy: 0.9825 - val_loss: 0.5533 - val_accuracy: 0.8698\n",
      "\n",
      "Epoch 00155: val_loss did not improve from 0.45702\n",
      "Epoch 156/200\n",
      "1372/1372 [==============================] - 11s 8ms/step - loss: 0.0387 - accuracy: 0.9876 - val_loss: 0.5500 - val_accuracy: 0.8772\n",
      "\n",
      "Epoch 00156: val_loss did not improve from 0.45702\n",
      "Epoch 157/200\n",
      "1372/1372 [==============================] - 11s 8ms/step - loss: 0.0432 - accuracy: 0.9905 - val_loss: 0.5839 - val_accuracy: 0.8565\n",
      "\n",
      "Epoch 00157: val_loss did not improve from 0.45702\n",
      "Epoch 158/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1372/1372 [==============================] - 11s 8ms/step - loss: 0.0610 - accuracy: 0.9789 - val_loss: 0.6697 - val_accuracy: 0.8447\n",
      "\n",
      "Epoch 00158: val_loss did not improve from 0.45702\n",
      "Epoch 159/200\n",
      "1372/1372 [==============================] - 11s 8ms/step - loss: 0.0412 - accuracy: 0.9832 - val_loss: 0.5949 - val_accuracy: 0.8654\n",
      "\n",
      "Epoch 00159: val_loss did not improve from 0.45702\n",
      "Epoch 160/200\n",
      "1372/1372 [==============================] - 11s 8ms/step - loss: 0.0391 - accuracy: 0.9854 - val_loss: 0.5905 - val_accuracy: 0.8595\n",
      "\n",
      "Epoch 00160: val_loss did not improve from 0.45702\n",
      "Epoch 161/200\n",
      "1372/1372 [==============================] - 11s 8ms/step - loss: 0.0590 - accuracy: 0.9789 - val_loss: 0.6504 - val_accuracy: 0.8373\n",
      "\n",
      "Epoch 00161: val_loss did not improve from 0.45702\n",
      "\n",
      "Epoch 00161: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 162/200\n",
      "1372/1372 [==============================] - 11s 8ms/step - loss: 0.0369 - accuracy: 0.9891 - val_loss: 0.5253 - val_accuracy: 0.8609\n",
      "\n",
      "Epoch 00162: val_loss did not improve from 0.45702\n",
      "Epoch 163/200\n",
      "1372/1372 [==============================] - 11s 8ms/step - loss: 0.0152 - accuracy: 0.9956 - val_loss: 0.5260 - val_accuracy: 0.8639\n",
      "\n",
      "Epoch 00163: val_loss did not improve from 0.45702\n",
      "Epoch 164/200\n",
      "1372/1372 [==============================] - 11s 8ms/step - loss: 0.0165 - accuracy: 0.9964 - val_loss: 0.5365 - val_accuracy: 0.8743\n",
      "\n",
      "Epoch 00164: val_loss did not improve from 0.45702\n",
      "Epoch 165/200\n",
      "1372/1372 [==============================] - 11s 8ms/step - loss: 0.0164 - accuracy: 0.9942 - val_loss: 0.5302 - val_accuracy: 0.8728\n",
      "\n",
      "Epoch 00165: val_loss did not improve from 0.45702\n",
      "Epoch 166/200\n",
      "1372/1372 [==============================] - 11s 8ms/step - loss: 0.0148 - accuracy: 0.9956 - val_loss: 0.5344 - val_accuracy: 0.8846\n",
      "\n",
      "Epoch 00166: val_loss did not improve from 0.45702\n",
      "Epoch 167/200\n",
      "1372/1372 [==============================] - 11s 8ms/step - loss: 0.0113 - accuracy: 0.9971 - val_loss: 0.5444 - val_accuracy: 0.8787\n",
      "\n",
      "Epoch 00167: val_loss did not improve from 0.45702\n",
      "Epoch 168/200\n",
      "1372/1372 [==============================] - 11s 8ms/step - loss: 0.0119 - accuracy: 0.9964 - val_loss: 0.5040 - val_accuracy: 0.8728\n",
      "\n",
      "Epoch 00168: val_loss did not improve from 0.45702\n",
      "Epoch 169/200\n",
      "1372/1372 [==============================] - 11s 8ms/step - loss: 0.0134 - accuracy: 0.9971 - val_loss: 0.5058 - val_accuracy: 0.8802\n",
      "\n",
      "Epoch 00169: val_loss did not improve from 0.45702\n",
      "Epoch 170/200\n",
      "1372/1372 [==============================] - 11s 8ms/step - loss: 0.0111 - accuracy: 0.9964 - val_loss: 0.5183 - val_accuracy: 0.8802\n",
      "\n",
      "Epoch 00170: val_loss did not improve from 0.45702\n",
      "Epoch 171/200\n",
      "1372/1372 [==============================] - 11s 8ms/step - loss: 0.0092 - accuracy: 0.9985 - val_loss: 0.5291 - val_accuracy: 0.8728\n",
      "\n",
      "Epoch 00171: val_loss did not improve from 0.45702\n",
      "Epoch 172/200\n",
      "1372/1372 [==============================] - 11s 8ms/step - loss: 0.0112 - accuracy: 0.9964 - val_loss: 0.5409 - val_accuracy: 0.8846\n",
      "\n",
      "Epoch 00172: val_loss did not improve from 0.45702\n",
      "Epoch 173/200\n",
      "1372/1372 [==============================] - 11s 8ms/step - loss: 0.0158 - accuracy: 0.9956 - val_loss: 0.5318 - val_accuracy: 0.8817\n",
      "\n",
      "Epoch 00173: val_loss did not improve from 0.45702\n",
      "Epoch 174/200\n",
      "1372/1372 [==============================] - 11s 8ms/step - loss: 0.0168 - accuracy: 0.9934 - val_loss: 0.5463 - val_accuracy: 0.8772\n",
      "\n",
      "Epoch 00174: val_loss did not improve from 0.45702\n",
      "Epoch 175/200\n",
      "1372/1372 [==============================] - 11s 8ms/step - loss: 0.0263 - accuracy: 0.9934 - val_loss: 0.5356 - val_accuracy: 0.8831\n",
      "\n",
      "Epoch 00175: val_loss did not improve from 0.45702\n",
      "Epoch 176/200\n",
      "1372/1372 [==============================] - 11s 8ms/step - loss: 0.0201 - accuracy: 0.9942 - val_loss: 0.5709 - val_accuracy: 0.8772\n",
      "\n",
      "Epoch 00176: val_loss did not improve from 0.45702\n",
      "Epoch 177/200\n",
      "1372/1372 [==============================] - 11s 8ms/step - loss: 0.0190 - accuracy: 0.9942 - val_loss: 0.5080 - val_accuracy: 0.8817\n",
      "\n",
      "Epoch 00177: val_loss did not improve from 0.45702\n",
      "Epoch 178/200\n",
      "1372/1372 [==============================] - 11s 8ms/step - loss: 0.0184 - accuracy: 0.9942 - val_loss: 0.4999 - val_accuracy: 0.8861\n",
      "\n",
      "Epoch 00178: val_loss did not improve from 0.45702\n",
      "Epoch 179/200\n",
      "1372/1372 [==============================] - 11s 8ms/step - loss: 0.0116 - accuracy: 0.9971 - val_loss: 0.5522 - val_accuracy: 0.8728\n",
      "\n",
      "Epoch 00179: val_loss did not improve from 0.45702\n",
      "Epoch 180/200\n",
      "1372/1372 [==============================] - 11s 8ms/step - loss: 0.0121 - accuracy: 0.9971 - val_loss: 0.5407 - val_accuracy: 0.8713\n",
      "\n",
      "Epoch 00180: val_loss did not improve from 0.45702\n",
      "Epoch 181/200\n",
      "1372/1372 [==============================] - 11s 8ms/step - loss: 0.0186 - accuracy: 0.9949 - val_loss: 0.5431 - val_accuracy: 0.8728\n",
      "\n",
      "Epoch 00181: val_loss did not improve from 0.45702\n",
      "Epoch 182/200\n",
      "1372/1372 [==============================] - 11s 8ms/step - loss: 0.0139 - accuracy: 0.9949 - val_loss: 0.5679 - val_accuracy: 0.8757\n",
      "\n",
      "Epoch 00182: val_loss did not improve from 0.45702\n",
      "Epoch 183/200\n",
      "1372/1372 [==============================] - 11s 8ms/step - loss: 0.0143 - accuracy: 0.9956 - val_loss: 0.5382 - val_accuracy: 0.8728\n",
      "\n",
      "Epoch 00183: val_loss did not improve from 0.45702\n",
      "Epoch 184/200\n",
      "1372/1372 [==============================] - 11s 8ms/step - loss: 0.0146 - accuracy: 0.9949 - val_loss: 0.5311 - val_accuracy: 0.8757\n",
      "\n",
      "Epoch 00184: val_loss did not improve from 0.45702\n",
      "Epoch 185/200\n",
      "1372/1372 [==============================] - 11s 8ms/step - loss: 0.0142 - accuracy: 0.9949 - val_loss: 0.5466 - val_accuracy: 0.8698\n",
      "\n",
      "Epoch 00185: val_loss did not improve from 0.45702\n",
      "Epoch 186/200\n",
      "1372/1372 [==============================] - 11s 8ms/step - loss: 0.0149 - accuracy: 0.9942 - val_loss: 0.5677 - val_accuracy: 0.8802\n",
      "\n",
      "Epoch 00186: val_loss did not improve from 0.45702\n",
      "Epoch 187/200\n",
      "1372/1372 [==============================] - 11s 8ms/step - loss: 0.0201 - accuracy: 0.9927 - val_loss: 0.5669 - val_accuracy: 0.8757\n",
      "\n",
      "Epoch 00187: val_loss did not improve from 0.45702\n",
      "Epoch 188/200\n",
      "1372/1372 [==============================] - 13s 9ms/step - loss: 0.0113 - accuracy: 0.9978 - val_loss: 0.5564 - val_accuracy: 0.8831\n",
      "\n",
      "Epoch 00188: val_loss did not improve from 0.45702\n",
      "Epoch 189/200\n",
      "1372/1372 [==============================] - 11s 8ms/step - loss: 0.0107 - accuracy: 0.9978 - val_loss: 0.5613 - val_accuracy: 0.8831\n",
      "\n",
      "Epoch 00189: val_loss did not improve from 0.45702\n",
      "Epoch 190/200\n",
      "1372/1372 [==============================] - 11s 8ms/step - loss: 0.0120 - accuracy: 0.9985 - val_loss: 0.5717 - val_accuracy: 0.8728\n",
      "\n",
      "Epoch 00190: val_loss did not improve from 0.45702\n",
      "Epoch 191/200\n",
      "1372/1372 [==============================] - 11s 8ms/step - loss: 0.0139 - accuracy: 0.9942 - val_loss: 0.5819 - val_accuracy: 0.8817\n",
      "\n",
      "Epoch 00191: val_loss did not improve from 0.45702\n",
      "Epoch 192/200\n",
      "1372/1372 [==============================] - 11s 8ms/step - loss: 0.0173 - accuracy: 0.9934 - val_loss: 0.5969 - val_accuracy: 0.8831\n",
      "\n",
      "Epoch 00192: val_loss did not improve from 0.45702\n",
      "Epoch 193/200\n",
      "1372/1372 [==============================] - 11s 8ms/step - loss: 0.0226 - accuracy: 0.9934 - val_loss: 0.5837 - val_accuracy: 0.8905\n",
      "\n",
      "Epoch 00193: val_loss did not improve from 0.45702\n",
      "Epoch 194/200\n",
      "1372/1372 [==============================] - 11s 8ms/step - loss: 0.0183 - accuracy: 0.9949 - val_loss: 0.5564 - val_accuracy: 0.8787\n",
      "\n",
      "Epoch 00194: val_loss did not improve from 0.45702\n",
      "Epoch 195/200\n",
      "1372/1372 [==============================] - 11s 8ms/step - loss: 0.0190 - accuracy: 0.9913 - val_loss: 0.5856 - val_accuracy: 0.8743\n",
      "\n",
      "Epoch 00195: val_loss did not improve from 0.45702\n",
      "Epoch 196/200\n",
      "1372/1372 [==============================] - 11s 8ms/step - loss: 0.0144 - accuracy: 0.9949 - val_loss: 0.5790 - val_accuracy: 0.8802\n",
      "\n",
      "Epoch 00196: val_loss did not improve from 0.45702\n",
      "Epoch 197/200\n",
      "1372/1372 [==============================] - 11s 8ms/step - loss: 0.0119 - accuracy: 0.9964 - val_loss: 0.6051 - val_accuracy: 0.8846\n",
      "\n",
      "Epoch 00197: val_loss did not improve from 0.45702\n",
      "Epoch 198/200\n",
      "1372/1372 [==============================] - 11s 8ms/step - loss: 0.0169 - accuracy: 0.9927 - val_loss: 0.5939 - val_accuracy: 0.8802\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00198: val_loss did not improve from 0.45702\n",
      "Epoch 199/200\n",
      "1372/1372 [==============================] - 11s 8ms/step - loss: 0.0082 - accuracy: 0.9978 - val_loss: 0.5326 - val_accuracy: 0.8876\n",
      "\n",
      "Epoch 00199: val_loss did not improve from 0.45702\n",
      "Epoch 200/200\n",
      "1372/1372 [==============================] - 11s 8ms/step - loss: 0.0089 - accuracy: 0.9978 - val_loss: 0.5439 - val_accuracy: 0.8891\n",
      "\n",
      "Epoch 00200: val_loss did not improve from 0.45702\n"
     ]
    }
   ],
   "source": [
    "# 훈련시키면서 가장 정확도가 높은 모델 저장하는 방법 : ModelCheckpoint\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "reLR = ReduceLROnPlateau(patience=50,verbose=1,factor=0.5) #learning rate scheduler\n",
    "es = EarlyStopping(patience=100, verbose=1)\n",
    "checkpointer = ModelCheckpoint(filepath='D:/DACON_MNIST/data/model.weights.best.hdf5', verbose = 1, save_best_only=True)\n",
    "history = model.fit(x_train2,\n",
    "          y_train2,\n",
    "          batch_size=32,\n",
    "          epochs=200,\n",
    "          validation_data=(x_valid2, y_valid2),\n",
    "          callbacks=[checkpointer,es,reLR])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 진행한 결과 모델 저장하기\n",
    "from keras.models import load_model\n",
    "\n",
    "model.save('D:/DACON_MNIST/data/DACON_mnist_model1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.99489796, 0.9963557, 0.99271137, 0.9978134, 0.9978134]\n"
     ]
    }
   ],
   "source": [
    "# 모델 학습 결과 그래프로 확인하기\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.title('Model loss & accuracy')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['loss', 'accuracy'], loc='upper left')\n",
    "plt.show()\n",
    "print(history.history['accuracy'][-5:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>digit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2049</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2050</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2051</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2052</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2053</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id  digit\n",
       "0  2049      6\n",
       "1  2050      9\n",
       "2  2051      8\n",
       "3  2052      0\n",
       "4  2053      3"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test 데이터셋에서 x_train과 동일한 조건으로 예측 실행\n",
    "x_test = test.drop(['id', 'letter'], axis=1).values\n",
    "x_test = x_test.reshape(-1, 28, 28, 1)\n",
    "x_test = x_test/255\n",
    "\n",
    "# submission의 'digit'에 예측한 결과값을 적어줌\n",
    "submission = pd.read_csv('D:/DACON_MNIST/data/submission.csv')\n",
    "submission['digit'] = np.argmax(model.predict(x_test), axis=1)\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결과를 csv파일로 저장함\n",
    "submission.to_csv('D:/DACON_MNIST/data/DACON_MNIST_02.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "test"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
