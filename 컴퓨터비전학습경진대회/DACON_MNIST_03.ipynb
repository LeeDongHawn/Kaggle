{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DACON_컴퓨터비전학습경진대회 연습\n",
    "## 정확도 90%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical CPU, 1 Logical CPU\n"
     ]
    }
   ],
   "source": [
    "# GPU 사용을 위한 코드\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0' # Set to -1 if CPU should be used CPU = -1 , GPU = 0\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "cpus = tf.config.experimental.list_physical_devices('CPU')\n",
    "\n",
    "if gpus:\n",
    "    try:\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)\n",
    "elif cpus:\n",
    "    try:\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        logical_cpus= tf.config.experimental.list_logical_devices('CPU')\n",
    "        print(len(cpus), \"Physical CPU,\", len(logical_cpus), \"Logical CPU\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "train = pd.read_csv('D:/DACON_MNIST/data/train.csv')\n",
    "test = pd.read_csv('D:/DACON_MNIST/data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id  digit letter  0  1  2  3  4  5  6  ...  774  775  776  777  778  779  \\\n",
      "0   1      5      L  1  1  1  4  3  0  0  ...    2    1    0    1    2    4   \n",
      "1   2      0      B  0  4  0  0  4  1  1  ...    0    3    0    1    4    1   \n",
      "2   3      4      L  1  1  2  2  1  1  1  ...    3    3    3    0    2    0   \n",
      "3   4      9      D  1  2  0  2  0  4  0  ...    3    3    2    0    1    4   \n",
      "4   5      6      A  3  0  2  4  0  3  0  ...    4    4    3    2    1    3   \n",
      "\n",
      "   780  781  782  783  \n",
      "0    4    4    3    4  \n",
      "1    4    2    1    2  \n",
      "2    3    0    2    2  \n",
      "3    0    0    1    1  \n",
      "4    4    3    1    2  \n",
      "\n",
      "[5 rows x 787 columns]\n",
      "     id letter  0  1  2  3  4  5  6  7  ...  774  775  776  777  778  779  \\\n",
      "0  2049      L  0  4  0  2  4  2  3  1  ...    2    0    4    2    2    4   \n",
      "1  2050      C  4  1  4  0  1  1  0  2  ...    0    3    2    4    2    4   \n",
      "2  2051      S  0  4  0  1  3  2  3  0  ...    1    3    2    0    3    2   \n",
      "3  2052      K  2  1  3  3  3  4  3  0  ...    3    0    3    2    4    1   \n",
      "4  2053      W  1  0  1  1  2  2  1  4  ...    4    3    1    4    0    2   \n",
      "\n",
      "   780  781  782  783  \n",
      "0    3    4    1    4  \n",
      "1    2    2    1    2  \n",
      "2    3    0    1    4  \n",
      "3    0    4    4    4  \n",
      "4    1    2    3    4  \n",
      "\n",
      "[5 rows x 786 columns]\n",
      "2048\n",
      "20480\n"
     ]
    }
   ],
   "source": [
    "# train data는 id, digit, letter, pixel 로 구성되어 있다.\n",
    "print(train.head(5))\n",
    "# test data는 id, letter, pixel로 구성되어 있다.\n",
    "print(test.head(5))\n",
    "# submission은 id와 digit로 구성되어 있다.\n",
    "print(len(train))\n",
    "print(len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# train 데이터에 있는 이미지 출력해보기(Index에 있는 이미지, 이미지에 해당하는 숫자와 문자)\n",
    "idx = 30\n",
    "img = train.loc[idx, '0':].values.reshape(28, 28).astype(int)\n",
    "digit = train.loc[idx, 'digit']\n",
    "letter = train.loc[idx, 'letter']\n",
    "\n",
    "plt.title('Index: %i, Digit: %s, Letter: %s'%(idx, digit, letter))\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "# Kfold - train과 test로만 나눔(사용하지 않을 column drop시킴)\n",
    "train2 = train.drop(['id', 'digit', 'letter'], axis=1).values\n",
    "test2 = test.drop(['id','letter'], axis=1).values\n",
    "\n",
    "# reshape\n",
    "train2 = train2.reshape(-1,28,28,1)\n",
    "test2 = test2.reshape(-1,28,28,1)\n",
    "\n",
    "# normalization\n",
    "train2 = train2/255.0\n",
    "test2 = test2/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이미지 Augmentation(test generator는 valid와 test에 사용됨)\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "idg_train = ImageDataGenerator(height_shift_range=(-1,1),width_shift_range=(-1,1))\n",
    "idg_test = ImageDataGenerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "# New - StratifiedKFold : fold 안의 데이터셋의 label 분포가 전체 데이터셋의 label분포를 따름\n",
    "# 각 fold가 전체 데이터셋을 잘 대표함\n",
    "# n_splits : fold의 개수, shuffle : 샘플 순서 랜덤, random_states : 고정해서 ㄸ고같은 작업을 재현할 수 있음)\n",
    "skf = StratifiedKFold(n_splits=2, random_state=10, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "52/52 [==============================] - 16s 309ms/step - loss: 2.9527 - accuracy: 0.1258 - val_loss: 2.7280 - val_accuracy: 0.1000\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 2.72800, saving model to D:/DACON_MNIST/data/model.weights.best.hdf5\n",
      "Epoch 2/200\n",
      "52/52 [==============================] - 14s 273ms/step - loss: 2.4643 - accuracy: 0.1966 - val_loss: 2.8844 - val_accuracy: 0.1000\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 2.72800\n",
      "Epoch 3/200\n",
      "52/52 [==============================] - 14s 274ms/step - loss: 2.2106 - accuracy: 0.2485 - val_loss: 3.5093 - val_accuracy: 0.1000\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 2.72800\n",
      "Epoch 4/200\n",
      "52/52 [==============================] - 14s 277ms/step - loss: 2.0770 - accuracy: 0.2845 - val_loss: 5.2710 - val_accuracy: 0.1000\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 2.72800\n",
      "Epoch 5/200\n",
      "52/52 [==============================] - 14s 273ms/step - loss: 1.9217 - accuracy: 0.3278 - val_loss: 6.1445 - val_accuracy: 0.1000\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 2.72800\n",
      "Epoch 6/200\n",
      "52/52 [==============================] - 15s 283ms/step - loss: 1.7680 - accuracy: 0.3663 - val_loss: 5.6811 - val_accuracy: 0.1000\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 2.72800\n",
      "Epoch 7/200\n",
      "52/52 [==============================] - 15s 282ms/step - loss: 1.6461 - accuracy: 0.4072 - val_loss: 5.2277 - val_accuracy: 0.1000\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 2.72800\n",
      "Epoch 8/200\n",
      "52/52 [==============================] - 15s 295ms/step - loss: 1.6245 - accuracy: 0.4414 - val_loss: 5.5865 - val_accuracy: 0.1000\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 2.72800\n",
      "Epoch 9/200\n",
      "52/52 [==============================] - 15s 291ms/step - loss: 1.4768 - accuracy: 0.4835 - val_loss: 6.5062 - val_accuracy: 0.1000\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 2.72800\n",
      "Epoch 10/200\n",
      "52/52 [==============================] - 16s 315ms/step - loss: 1.3759 - accuracy: 0.5275 - val_loss: 7.4940 - val_accuracy: 0.1000\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 2.72800\n",
      "Epoch 11/200\n",
      "52/52 [==============================] - 15s 283ms/step - loss: 1.2969 - accuracy: 0.5641 - val_loss: 5.3700 - val_accuracy: 0.1000\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 2.72800\n",
      "Epoch 12/200\n",
      "52/52 [==============================] - 15s 280ms/step - loss: 1.2035 - accuracy: 0.5824 - val_loss: 5.6855 - val_accuracy: 0.1000\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 2.72800\n",
      "Epoch 13/200\n",
      "52/52 [==============================] - 15s 282ms/step - loss: 1.1013 - accuracy: 0.6306 - val_loss: 4.4950 - val_accuracy: 0.1244\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 2.72800\n",
      "Epoch 14/200\n",
      "52/52 [==============================] - 14s 271ms/step - loss: 1.0804 - accuracy: 0.6404 - val_loss: 3.4124 - val_accuracy: 0.2073\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 2.72800\n",
      "Epoch 15/200\n",
      "52/52 [==============================] - 14s 277ms/step - loss: 0.9942 - accuracy: 0.6636 - val_loss: 1.8046 - val_accuracy: 0.4927\n",
      "\n",
      "Epoch 00015: val_loss improved from 2.72800 to 1.80460, saving model to D:/DACON_MNIST/data/model.weights.best.hdf5\n",
      "Epoch 16/200\n",
      "52/52 [==============================] - 15s 289ms/step - loss: 0.9402 - accuracy: 0.6746 - val_loss: 1.9599 - val_accuracy: 0.4073\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 1.80460\n",
      "Epoch 17/200\n",
      "52/52 [==============================] - 15s 282ms/step - loss: 0.8792 - accuracy: 0.7149 - val_loss: 1.7484 - val_accuracy: 0.4488\n",
      "\n",
      "Epoch 00017: val_loss improved from 1.80460 to 1.74840, saving model to D:/DACON_MNIST/data/model.weights.best.hdf5\n",
      "Epoch 18/200\n",
      "52/52 [==============================] - 14s 271ms/step - loss: 0.8175 - accuracy: 0.7271 - val_loss: 1.6121 - val_accuracy: 0.5829\n",
      "\n",
      "Epoch 00018: val_loss improved from 1.74840 to 1.61210, saving model to D:/DACON_MNIST/data/model.weights.best.hdf5\n",
      "Epoch 19/200\n",
      "52/52 [==============================] - 14s 272ms/step - loss: 0.7984 - accuracy: 0.7308 - val_loss: 1.0183 - val_accuracy: 0.7098\n",
      "\n",
      "Epoch 00019: val_loss improved from 1.61210 to 1.01835, saving model to D:/DACON_MNIST/data/model.weights.best.hdf5\n",
      "Epoch 20/200\n",
      "52/52 [==============================] - 14s 270ms/step - loss: 0.8038 - accuracy: 0.7436 - val_loss: 0.7579 - val_accuracy: 0.7171\n",
      "\n",
      "Epoch 00020: val_loss improved from 1.01835 to 0.75795, saving model to D:/DACON_MNIST/data/model.weights.best.hdf5\n",
      "Epoch 21/200\n",
      "52/52 [==============================] - 14s 272ms/step - loss: 0.7139 - accuracy: 0.7662 - val_loss: 1.5261 - val_accuracy: 0.5122\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.75795\n",
      "Epoch 22/200\n",
      "52/52 [==============================] - 14s 272ms/step - loss: 0.8268 - accuracy: 0.7344 - val_loss: 0.9088 - val_accuracy: 0.6634\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.75795\n",
      "Epoch 23/200\n",
      "52/52 [==============================] - 14s 271ms/step - loss: 0.7480 - accuracy: 0.7558 - val_loss: 0.5918 - val_accuracy: 0.7610\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.75795 to 0.59178, saving model to D:/DACON_MNIST/data/model.weights.best.hdf5\n",
      "Epoch 24/200\n",
      "52/52 [==============================] - 14s 273ms/step - loss: 0.6823 - accuracy: 0.7705 - val_loss: 0.8904 - val_accuracy: 0.7537\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.59178\n",
      "Epoch 25/200\n",
      "52/52 [==============================] - 14s 270ms/step - loss: 0.6664 - accuracy: 0.7863 - val_loss: 0.9648 - val_accuracy: 0.8220\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.59178\n",
      "Epoch 26/200\n",
      "52/52 [==============================] - 14s 270ms/step - loss: 0.7282 - accuracy: 0.7753 - val_loss: 1.1339 - val_accuracy: 0.6610\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.59178\n",
      "Epoch 27/200\n",
      "52/52 [==============================] - 14s 271ms/step - loss: 0.5891 - accuracy: 0.8034 - val_loss: 0.2461 - val_accuracy: 0.8268\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.59178 to 0.24609, saving model to D:/DACON_MNIST/data/model.weights.best.hdf5\n",
      "Epoch 28/200\n",
      "52/52 [==============================] - 14s 278ms/step - loss: 0.5853 - accuracy: 0.8107 - val_loss: 0.7199 - val_accuracy: 0.8317\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.24609\n",
      "Epoch 29/200\n",
      "52/52 [==============================] - 15s 286ms/step - loss: 0.5416 - accuracy: 0.8266 - val_loss: 0.1122 - val_accuracy: 0.8512\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.24609 to 0.11219, saving model to D:/DACON_MNIST/data/model.weights.best.hdf5\n",
      "Epoch 30/200\n",
      "52/52 [==============================] - 16s 300ms/step - loss: 0.5374 - accuracy: 0.8266 - val_loss: 0.4462 - val_accuracy: 0.8220\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.11219\n",
      "Epoch 31/200\n",
      "52/52 [==============================] - 16s 300ms/step - loss: 0.5243 - accuracy: 0.8327 - val_loss: 1.2286 - val_accuracy: 0.7951\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.11219\n",
      "Epoch 32/200\n",
      "52/52 [==============================] - 15s 286ms/step - loss: 0.5264 - accuracy: 0.8248 - val_loss: 1.4814 - val_accuracy: 0.6659\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.11219\n",
      "Epoch 33/200\n",
      "52/52 [==============================] - 14s 277ms/step - loss: 0.4814 - accuracy: 0.8474 - val_loss: 0.4228 - val_accuracy: 0.8220\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.11219\n",
      "Epoch 34/200\n",
      "52/52 [==============================] - 15s 282ms/step - loss: 0.4907 - accuracy: 0.8309 - val_loss: 0.2309 - val_accuracy: 0.8610\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.11219\n",
      "Epoch 35/200\n",
      "52/52 [==============================] - 15s 295ms/step - loss: 0.5223 - accuracy: 0.8388 - val_loss: 0.4964 - val_accuracy: 0.8463\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.11219\n",
      "Epoch 36/200\n",
      "52/52 [==============================] - 15s 281ms/step - loss: 0.4400 - accuracy: 0.8559 - val_loss: 0.8404 - val_accuracy: 0.8073\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.11219\n",
      "Epoch 37/200\n",
      "52/52 [==============================] - 15s 285ms/step - loss: 0.4418 - accuracy: 0.8535 - val_loss: 0.2481 - val_accuracy: 0.8439\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.11219\n",
      "Epoch 38/200\n",
      "52/52 [==============================] - 15s 289ms/step - loss: 0.4183 - accuracy: 0.8602 - val_loss: 0.6091 - val_accuracy: 0.8683\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.11219\n",
      "Epoch 39/200\n",
      "52/52 [==============================] - 15s 288ms/step - loss: 0.4027 - accuracy: 0.8663 - val_loss: 0.4087 - val_accuracy: 0.8561\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00039: val_loss did not improve from 0.11219\n",
      "Epoch 40/200\n",
      "52/52 [==============================] - 15s 281ms/step - loss: 0.4114 - accuracy: 0.8840 - val_loss: 0.3702 - val_accuracy: 0.8902\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.11219\n",
      "Epoch 41/200\n",
      "52/52 [==============================] - 15s 281ms/step - loss: 0.3844 - accuracy: 0.8675 - val_loss: 0.8669 - val_accuracy: 0.8293\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.11219\n",
      "Epoch 42/200\n",
      "52/52 [==============================] - 14s 277ms/step - loss: 0.3338 - accuracy: 0.8828 - val_loss: 0.4067 - val_accuracy: 0.8805\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.11219\n",
      "Epoch 43/200\n",
      "52/52 [==============================] - 14s 273ms/step - loss: 0.3392 - accuracy: 0.8834 - val_loss: 0.5544 - val_accuracy: 0.7659\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.11219\n",
      "Epoch 44/200\n",
      "52/52 [==============================] - 15s 279ms/step - loss: 0.4228 - accuracy: 0.8657 - val_loss: 0.4647 - val_accuracy: 0.8707\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.11219\n",
      "\n",
      "Epoch 00044: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 45/200\n",
      "52/52 [==============================] - 15s 285ms/step - loss: 0.3218 - accuracy: 0.8919 - val_loss: 0.2529 - val_accuracy: 0.8878\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.11219\n",
      "Epoch 46/200\n",
      "52/52 [==============================] - 15s 283ms/step - loss: 0.3016 - accuracy: 0.9042 - val_loss: 0.4490 - val_accuracy: 0.8561\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.11219\n",
      "Epoch 47/200\n",
      "52/52 [==============================] - 14s 275ms/step - loss: 0.2877 - accuracy: 0.9005 - val_loss: 0.3654 - val_accuracy: 0.8902\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.11219\n",
      "Epoch 48/200\n",
      "52/52 [==============================] - 14s 276ms/step - loss: 0.2760 - accuracy: 0.9066 - val_loss: 0.2736 - val_accuracy: 0.8927\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.11219\n",
      "Epoch 49/200\n",
      "52/52 [==============================] - 15s 283ms/step - loss: 0.2440 - accuracy: 0.9194 - val_loss: 0.1474 - val_accuracy: 0.8976\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.11219\n",
      "Epoch 00049: early stopping\n",
      "640/640 [==============================] - 35s 55ms/step\n",
      "1 번째 학습을 완료했습니다.\n",
      "Epoch 1/200\n",
      "52/52 [==============================] - 16s 314ms/step - loss: 3.0927 - accuracy: 0.1111 - val_loss: 2.5280 - val_accuracy: 0.0976\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 2.52802, saving model to D:/DACON_MNIST/data/model.weights.best.hdf5\n",
      "Epoch 2/200\n",
      "52/52 [==============================] - 15s 288ms/step - loss: 2.6293 - accuracy: 0.1410 - val_loss: 3.6692 - val_accuracy: 0.0976\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 2.52802\n",
      "Epoch 3/200\n",
      "52/52 [==============================] - 15s 285ms/step - loss: 2.3297 - accuracy: 0.2137 - val_loss: 3.6591 - val_accuracy: 0.0976\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 2.52802\n",
      "Epoch 4/200\n",
      " 8/52 [===>..........................] - ETA: 12s - loss: 2.1385 - accuracy: 0.2656"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-0b556755f488>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m     \u001b[1;31m# 모델 학습 수행\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 71\u001b[1;33m     \u001b[0mlearning_history\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_generator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_generator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m200\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalid_generator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcheckpointer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mreLR\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     72\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m     \u001b[1;31m# predict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\test\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[0;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 91\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\test\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   1730\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1731\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1732\u001b[1;33m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m   1733\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\test\\lib\\site-packages\\keras\\engine\\training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m    218\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    219\u001b[0m                                             \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 220\u001b[1;33m                                             reset_metrics=False)\n\u001b[0m\u001b[0;32m    221\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    222\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\test\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[0;32m   1512\u001b[0m             \u001b[0mins\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1513\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1514\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1515\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1516\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\test\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   3290\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3291\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[1;32m-> 3292\u001b[1;33m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[0;32m   3293\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3294\u001b[0m     output_structure = nest.pack_sequence_as(\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\test\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[0;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1458\u001b[1;33m                                                run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1459\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 훈련시키면서 가장 정확도가 높은 모델 저장하는 방법 : ModelCheckpoint\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers import *\n",
    "from keras import Sequential\n",
    "\n",
    "reLR = ReduceLROnPlateau(patience=15,verbose=1,factor=0.5) #learning rate scheduler\n",
    "es = EarlyStopping(patience=20, verbose=1)\n",
    "\n",
    "val_loss_min = []\n",
    "result = 0\n",
    "nth = 0\n",
    "\n",
    "# train, valid set으로 나눠서 실행\n",
    "for train_index, valid_index in skf.split(train2,train['digit']) :\n",
    "    \n",
    "    checkpointer = ModelCheckpoint(filepath='D:/DACON_MNIST/data/model.weights.best.hdf5', verbose = 1, save_best_only=True)\n",
    "    \n",
    "    # train을 x_train, x_valid로 나눔\n",
    "    x_train = train2[train_index]\n",
    "    x_valid = train2[valid_index]    \n",
    "    # train y_train, y_valid로 나눔\n",
    "    y_train = train['digit'][train_index]\n",
    "    y_valid = train['digit'][valid_index]\n",
    "    \n",
    "    # data generator 적용\n",
    "    train_generator = idg_train.flow(x_train,y_train,batch_size=32)\n",
    "    valid_generator = idg_test.flow(x_valid,y_valid)\n",
    "    test_generator = idg_test.flow(test2,shuffle=False)\n",
    "    \n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Conv2D(16,(3,3),activation='relu',input_shape=(28,28,1),padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "    model.add(Conv2D(32,(3,3),activation='relu',padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(32,(5,5),activation='relu',padding='same')) \n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(32,(5,5),activation='relu',padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(32,(5,5),activation='relu',padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D((3,3)))\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "    model.add(Conv2D(64,(3,3),activation='relu',padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(64,(5,5),activation='relu',padding='same')) \n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D((3,3)))\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "    model.add(Flatten())\n",
    "\n",
    "    model.add(Dense(128,activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(64,activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "    model.add(Dense(10,activation='softmax'))\n",
    "\n",
    "    # 모델 compile(loss, optimizer 등 설정)\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    # 모델 학습 수행\n",
    "    learning_history = model.fit_generator(train_generator,epochs=200, validation_data=valid_generator,callbacks=[es,checkpointer,reLR])\n",
    "    \n",
    "    # predict\n",
    "    model.load_weights('D:/DACON_MNIST/data/model.weights.best.hdf5')\n",
    "    result += model.predict_generator(test_generator,verbose=True)/40\n",
    "    \n",
    "    # save val_loss\n",
    "    hist = pd.DataFrame(learning_history.history)\n",
    "    val_loss_min.append(hist['val_loss'].min())\n",
    "    \n",
    "    nth += 1\n",
    "    print(nth, '번째 학습을 완료했습니다.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(val_loss_min, np.mean(val_loss_min))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# submission의 'digit'에 예측한 결과값을 적어줌\n",
    "submission = pd.read_csv('D:/DACON_MNIST/data/submission.csv')\n",
    "submission['digit'] = result.argmax(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결과를 csv파일로 저장함\n",
    "submission.to_csv('D:/DACON_MNIST/data/DACON_MNIST_03.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 학습 결과 그래프로 확인하기\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.plot(learning_history.history['loss'])\n",
    "plt.plot(learning_history.history['accuracy'])\n",
    "plt.title('Model loss & accuracy')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['loss', 'accuracy'], loc='upper left')\n",
    "plt.show()\n",
    "print(learning_history.history['accuracy'][-5:])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "test"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
